{"cells":[{"cell_type":"code","metadata":{"cell_id":"0dca4d07c84d47cba2a651d89a5658db","deepnote_cell_type":"code"},"source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import matrix_rank\nfrom matplotlib.lines import Line2D\nimport sys\nimport os\n\nif not os.environ.get('DISPLAY'):\n    plt.switch_backend('Agg')\n\n# LIF Network Class\nclass LIFNetwork:\n    \"\"\"Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) model.\n    \n    The connections are automatically generated as fully feedforward\n    between adjacent layers.\n    \"\"\"\n    def __init__(self, layers, tau=6.0, theta=0.4, reset=0.0, dt=0.001):\n        self.layers = layers\n        self.tau = tau\n        self.theta = theta\n        self.reset = reset\n        self.dt = dt\n        self.n_layers = len(layers)\n        self.n = sum(layers) # Total number of neurons\n        # Cumulative index of layers for easy slicing (e.g., [0, 1, 3, 4] for [1, 2, 1])\n        self.layer_idx = np.cumsum([0] + layers)\n\n        # Generic full feedforward connection generation (Post-synaptic index, Pre-synaptic index)\n        self.connections = []\n        for l in range(1, self.n_layers):\n            # Iterate over neurons in the post-synaptic layer (l)\n            for post in range(self.layer_idx[l], self.layer_idx[l+1]):\n                # Iterate over neurons in the pre-synaptic layer (l-1)\n                for pre in range(self.layer_idx[l-1], self.layer_idx[l]):\n                    self.connections.append((post, pre))\n\n    def simulate(self, x0, T, u):\n        \"\"\"Simulates the LIF network dynamics.\"\"\"\n        t = np.arange(0, T + self.dt, self.dt)\n        x = np.zeros((len(t), self.n)) # Neuron membrane potentials\n        x[0] = x0\n        spikes = [[] for _ in range(self.n)]\n\n        for i in range(len(t) - 1):\n            # dx is initialized as a float array for the update at time i\n            dx = -x[i].astype(float) / self.tau # Leak term\n            \n            # Synaptic input from connected neurons\n            # Note: In this simple model, connection weight is implicitly 1/tau\n            for post, pre in self.connections:\n                dx[post] += x[i, pre] / self.tau\n            \n            # External input (u) to the input layer (layer 0)\n            uval = np.array(u(t[i]), dtype=float) if callable(u) else np.array(u, dtype=float)\n            \n            # Ensure input dimensions match the input layer size\n            input_size = self.layers[0]\n            if len(uval) != input_size:\n                raise ValueError(f\"Input signal dimension ({len(uval)}) must match input layer size ({input_size}).\")\n            \n            dx[:input_size] += uval[:input_size] / self.tau \n            \n            # Euler integration step\n            x[i + 1] = x[i] + dx * self.dt\n            \n            # Check for spikes and reset\n            for idx in range(self.n):\n                if x[i + 1, idx] >= self.theta:\n                    spikes[idx].append(t[i + 1])\n                    x[i + 1, idx] = self.reset\n                    \n        return t, x, spikes\n\n    def controllability_observability(self):\n        \"\"\"Checks linear controllability and observability (approximate).\"\"\"\n        # A is the matrix defining the internal dynamics (Leak + Neuron-to-Neuron interaction)\n        A = -np.eye(self.n) / self.tau\n        \n        # Add connections based on the feedforward architecture\n        for post, pre in self.connections:\n            A[post, pre] += 1.0 / self.tau\n\n        # B is the input matrix (Input only affects the first layer)\n        B = np.zeros((self.n, self.layers[0]))\n        for i in range(self.layers[0]):\n            B[i, i] = 1.0 / self.tau\n            \n        # C is the output matrix (measuring all states)\n        C = np.eye(self.n)\n        \n        # Controllability Matrix (CM)\n        CM = B.copy()\n        for i in range(1, self.n):\n            # CM = [B, A*B, A^2*B, ...]\n            # Using matrix multiplication with A and the last block of B columns\n            CM = np.hstack((CM, A @ CM[:, -B.shape[1]:])) \n            \n        ctrl = matrix_rank(CM) == self.n\n        \n        # Observability Matrix (OM)\n        OM = C.copy()\n        for i in range(1, self.n):\n            # OM = [C^T, (C*A)^T, (C*A^2)^T, ...]^T\n            OM = np.vstack((OM, C @ np.linalg.matrix_power(A, i)))\n            \n        obs = matrix_rank(OM) == self.n\n        \n        return ctrl, obs\n\n# Draw architecture diagram\n\ndef draw_architecture(ax, layers, connections, title):\n    \"\"\"Draws the SNN architecture diagram.\"\"\"\n    \n    global_to_pos = {} \n    global_idx = 0\n    \n    x_positions = np.linspace(0.1, 0.9, len(layers))\n\n    colors = {\n        0: \"#3498DB\", # Input (Vibrant Blue)\n        -1: \"#2ECC71\", # Output (Vibrant Green)\n        \"hidden\": \"#E74C3C\" # Hidden (Vibrant Red/Rose)\n    }\n\n    # 1. Create nodes and map their global positions\n    for layer_index, count in enumerate(layers):\n        if layer_index == 0:\n            c = colors[0] \n        elif layer_index == len(layers) - 1:\n            c = colors[-1] \n        else:\n            c = colors[\"hidden\"]\n\n        # Adjust y-positions to center the layers better\n        y_positions = np.linspace(0.9, 0.1, count)\n        xs = np.full(count, x_positions[layer_index])\n\n        # Draw the nodes\n        ax.scatter(xs, y_positions, s=500, color=c, edgecolor='black', zorder=3)\n        \n        # Store global position mapping\n        for i, (x, y) in enumerate(zip(xs, y_positions)):\n            global_to_pos[global_idx] = (x, y)\n            global_idx += 1\n\n    # 2. Draw connections\n    for post, pre in connections:\n        if pre in global_to_pos and post in global_to_pos:\n            xa, ya = global_to_pos[pre] \n            xb, yb = global_to_pos[post]\n            # Draw line from pre to post\n            ax.plot([xa, xb], [ya, yb], color='gray', linewidth=0.8, zorder=2)\n\n    # 3. Title and Axis\n    ax.set_title(title, fontsize=13)\n    ax.axis('off')\n\n    # 4. Add color legend\n    legend_labels = [\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=10, label=\"Entrada (Input)\"),\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[\"hidden\"], markersize=10, label=\"Capa Oculta (Hidden)\"),\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[-1], markersize=10, label=\"Salida (Output)\")\n    ]\n    ax.legend(handles=legend_labels, loc='upper left', fontsize=10)\n\n# Plot experiment \n\ndef plot_experiment(sys, x0, T, input_signal, title):\n    \"\"\"Runs simulation and generates 5 independent figures.\"\"\"\n    print(f\"Running simulation for {title}...\")\n    t, x, spikes = sys.simulate(x0, T, input_signal)\n    ctrl, obs = sys.controllability_observability()\n    print(f\"Linear Controllability: {ctrl}, Observability: {obs}\")\n\n    # --- 1) Arquitectura (Architecture) ---\n    fig1, ax1 = plt.subplots(figsize=(10, 6))\n    draw_architecture(ax1, sys.layers, sys.connections, f\"{title} - Arquitectura (Estructura: {sys.layers})\")\n    fig1.tight_layout()\n    plt.show()\n\n    # --- 2) Entrada (Input) ---\n    fig2, ax2 = plt.subplots(figsize=(10, 3))\n    uvals = np.array([input_signal(ti) if callable(input_signal) else input_signal for ti in t])\n    \n    input_dim = sys.layers[0]\n    #flat case\n    if uvals.ndim == 1 and input_dim > 1:\n        uvals = uvals.reshape(-1, input_dim)\n    elif uvals.ndim == 1 and input_dim == 1:\n        # Single input, reshape to (T, 1) if necessary\n        uvals = uvals.reshape(-1, 1)\n\n    for i in range(input_dim):\n        ax2.plot(t, uvals[:, i], label=f\"Input {i+1}\", linestyle='--' if input_dim > 1 else '-')\n        \n    ax2.set_ylabel(\"Entrada\")\n    ax2.grid(True)\n    ax2.legend() \n    ax2.set_title(f\"Entrada | Linealmente Controlable: {ctrl}, Linealmente Observable: {obs}\")\n    ax2.set_xlabel(\"Tiempo (s)\")\n    fig2.tight_layout()\n    plt.show()\n\n    # --- 3) Potenciales de neuronas ocultas ---\n    fig3, ax3 = plt.subplots(figsize=(10, 4))\n    hidden_start = sys.layers[0]\n    hidden_end = sys.n - sys.layers[-1]\n    \n    if hidden_end > hidden_start: \n        for idx in range(hidden_start, hidden_end):\n            # Using high-contrast color palette\n            color_index = idx - hidden_start\n            color = plt.cm.get_cmap('Set1')(color_index % 9)\n            ax3.plot(t, x[:, idx], label=f\"Oculta {idx - hidden_start + 1}\", color=color)\n        \n        ax3.axhline(sys.theta, color='gray', linestyle=':', label='Umbral')\n\n    else: \n        ax3.text(0.5, 0.5, \"No hay capa oculta para mostrar\", ha='center', va='center')\n        \n    ax3.set_ylabel(\"Potencial Oculto\")\n    ax3.set_title(\"Potenciales de la capa oculta\")\n    ax3.grid(True)\n    ax3.legend()\n    ax3.set_xlabel(\"Tiempo (s)\")\n    fig3.tight_layout()\n    plt.show()\n\n    # --- 4) Membrana de salida + spikes (Output membrane + spikes) ---\n    fig4, ax4 = plt.subplots(figsize=(10, 4))\n    output_idx = sys.n - 1 # Assuming single output neuron\n    ax4.plot(t, x[:, output_idx], label=\"Membrana salida\", color='#2ECC71', linewidth=2)\n    ax4.axhline(sys.theta, color='#E74C3C', linestyle='--', label='Umbral')\n    \n    # Plot spikes slightly above the threshold line\n    ax4.eventplot(spikes[output_idx], lineoffsets=sys.theta + 0.05, colors='black', linelength=0.1)\n    ax4.set_ylabel(\"Potencial Salida\")\n    ax4.set_xlabel(\"Tiempo (s)\")\n    ax4.legend()\n    ax4.grid(True)\n    ax4.set_title(\"Membrana de salida y Umbral\")\n    fig4.tight_layout()\n    plt.show()\n\n    # --- 5) Spikes de salida solamente (Output spikes only) ---\n    fig5, ax5 = plt.subplots(figsize=(10, 2))\n    ax5.eventplot(spikes[output_idx], lineoffsets=1, colors='k')\n    ax5.set_ylabel(\"Spike\")\n    ax5.set_xlabel(\"Tiempo (s)\")\n    ax5.set_title(\"Spikes de salida\")\n    ax5.set_yticks([])\n    ax5.grid(True)\n    fig5.tight_layout()\n    plt.show()\n\n# ============================================================\n# Run single analogous experiment (2 -> 3 -> 1)\n# ============================================================\n\ndef main():\n    # Architectures commonly used in SNN control literature, prioritizing high controllability (large L0/n)\n    architectures_to_test = [\n        [3, 1, 1], # n=5, L0=3 (Best candidate for controllability)\n        [2, 2, 1], # n=5, L0=2 \n        [2, 3, 1], # n=6, L0=2\n        [3, 2, 1], # n=6, L0=3 \n        [4, 1, 1], # n=6, L0=4 (High controllability)\n        [1, 1, 1]  # n=3, L0=1 (Simple minimum structure)\n    ]\n    \n    T = 10.0  # Simulation time (seconds)\n    found_controllable = False\n\n    print(\"--- Searching for a Controllable LIF Architecture for Shooting Method Analogy ---\")\n    \n    for LAYERS in architectures_to_test:\n        sys_search = LIFNetwork(LAYERS, tau=10.0, theta=1.0, reset=0.0, dt=0.001)\n        # Check controllability without running the full simulation\n        ctrl, obs = sys_search.controllability_observability()\n        \n        print(f\"Checking architecture {LAYERS} (n={sys_search.n}, L0={LAYERS[0]}): Controllable={ctrl}\")\n        \n        if ctrl:\n            print(f\"\\n--- Found Controllable Architecture: {LAYERS} ---\")\n            \n            # Setup inputs for simulation\n            L0 = LAYERS[0]\n            x0 = np.zeros(sys_search.n)\n            # Input must match the found input size L0. Set constant input of 1.5 for all L0 inputs.\n            u_input = lambda t: [1.5] * L0\n            \n            # Run the full simulation and plotting for the first controllable system found\n            plot_experiment(sys_search, x0, T, u_input, \n                            f\"Controllable SNN Model Found: {LAYERS[0]}->{LAYERS[1]}->{LAYERS[2]}\")\n            \n            found_controllable = True\n            break # Stop after finding the first one\n            \n    if not found_controllable:\n        print(\"\\n--- Search completed. No controllable architecture found in the test list. ---\")\n\nif __name__ == \"__main__\":\n    main()","block_group":"f7d01e3a266043e4be6096b2119c2cbb","execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"--- Searching for a Controllable LIF Architecture for Shooting Method Analogy ---\nChecking architecture [3, 1, 1] (n=5, L0=3): Controllable=True\n\n--- Found Controllable Architecture: [3, 1, 1] ---\nRunning simulation for Controllable SNN Model Found: 3->1->1...\nLinear Controllability: True, Observability: True\n"},{"name":"stderr","output_type":"stream","text":"C:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:181: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\nC:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:204: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\nC:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:215: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  color = plt.cm.get_cmap('Set1')(color_index % 9)\nC:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:229: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\nC:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:245: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\nC:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\4188111312.py:256: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\n"}],"outputs_reference":"s3:deepnote-cell-outputs-production/18cc7017-7954-482a-8b43-d6d36ca0c3c1","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"7e7e4496a67548c1afff2f5bc5f7791b","deepnote_cell_type":"code"},"source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import matrix_rank\nfrom matplotlib.lines import Line2D\n\n# LIF Network Class\nclass LIFNetwork:\n    \"\"\"Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) model.\n    \n    The connections are automatically generated as fully feedforward\n    between adjacent layers.\n    \"\"\"\n    def __init__(self, layers, tau=6.0, theta=0.4, reset=0.0, dt=0.001):\n        self.layers = layers\n        self.tau = tau\n        self.theta = theta\n        self.reset = reset\n        self.dt = dt\n        self.n_layers = len(layers)\n        self.n = sum(layers) # Total number of neurons\n        # Cumulative index of layers for easy slicing (e.g., [0, 1, 3, 4] for [1, 2, 1])\n        self.layer_idx = np.cumsum([0] + layers)\n\n        # Generic full feedforward connection generation (Post-synaptic index, Pre-synaptic index)\n        self.connections = []\n        for l in range(1, self.n_layers):\n            # Iterate over neurons in the post-synaptic layer (l)\n            for post in range(self.layer_idx[l], self.layer_idx[l+1]):\n                # Iterate over neurons in the pre-synaptic layer (l-1)\n                for pre in range(self.layer_idx[l-1], self.layer_idx[l]):\n                    self.connections.append((post, pre))\n\n    def simulate(self, x0, T, u):\n        \"\"\"Simulates the LIF network dynamics.\"\"\"\n        t = np.arange(0, T + self.dt, self.dt)\n        x = np.zeros((len(t), self.n)) # Neuron membrane potentials\n        x[0] = x0\n        spikes = [[] for _ in range(self.n)]\n\n        for i in range(len(t) - 1):\n            # dx is initialized as a float array for the update at time i\n            dx = -x[i].astype(float) / self.tau # Leak term\n            \n            # Synaptic input from connected neurons\n            # Note: In this simple model, connection weight is implicitly 1/tau\n            for post, pre in self.connections:\n                dx[post] += x[i, pre] / self.tau\n            \n            # External input (u) to the input layer (layer 0)\n            uval = np.array(u(t[i]), dtype=float) if callable(u) else np.array(u, dtype=float)\n            \n            # Ensure input dimensions match the input layer size\n            input_size = self.layers[0]\n            if len(uval) != input_size:\n                raise ValueError(f\"Input signal dimension ({len(uval)}) must match input layer size ({input_size}).\")\n            \n            dx[:input_size] += uval[:input_size] / self.tau \n            \n            # Euler integration step\n            x[i + 1] = x[i] + dx * self.dt\n            \n            # Check for spikes and reset\n            for idx in range(self.n):\n                if x[i + 1, idx] >= self.theta:\n                    spikes[idx].append(t[i + 1])\n                    x[i + 1, idx] = self.reset\n                    \n        return t, x, spikes\n\n    def controllability_observability(self):\n        \"\"\"Checks linear controllability and observability (approximate).\"\"\"\n        # A is the matrix defining the internal dynamics (Leak + Neuron-to-Neuron interaction)\n        A = -np.eye(self.n) / self.tau\n        \n        # Add connections based on the feedforward architecture\n        for post, pre in self.connections:\n            A[post, pre] += 1.0 / self.tau\n\n        # B is the input matrix (Input only affects the first layer)\n        B = np.zeros((self.n, self.layers[0]))\n        for i in range(self.layers[0]):\n            B[i, i] = 1.0 / self.tau\n            \n        # C is the output matrix (measuring all states)\n        C = np.eye(self.n)\n        \n        # Controllability Matrix (CM)\n        CM = B.copy()\n        for i in range(1, self.n):\n            # CM = [B, A*B, A^2*B, ...]\n            # Using matrix multiplication with A and the last block of B columns\n            CM = np.hstack((CM, A @ CM[:, -B.shape[1]:])) \n            \n        ctrl = matrix_rank(CM) == self.n\n        \n        # Observability Matrix (OM)\n        OM = C.copy()\n        for i in range(1, self.n):\n            # OM = [C^T, (C*A)^T, (C*A^2)^T, ...]^T\n            OM = np.vstack((OM, C @ np.linalg.matrix_power(A, i)))\n            \n        obs = matrix_rank(OM) == self.n\n        \n        return ctrl, obs\n\n# Draw architecture diagram\n\ndef draw_architecture(ax, layers, connections, title):\n    \"\"\"Draws the SNN architecture diagram.\"\"\"\n    \n    global_to_pos = {} \n    global_idx = 0\n    \n    x_positions = np.linspace(0.1, 0.9, len(layers))\n\n    colors = {\n        0: \"#3498DB\", # Input (Vibrant Blue)\n        -1: \"#2ECC71\", # Output (Vibrant Green)\n        \"hidden\": \"#E74C3C\" # Hidden (Vibrant Red/Rose)\n    }\n\n    # 1. Create nodes and map their global positions\n    for layer_index, count in enumerate(layers):\n        if layer_index == 0:\n            c = colors[0] \n        elif layer_index == len(layers) - 1:\n            c = colors[-1] \n        else:\n            c = colors[\"hidden\"]\n\n        # Adjust y-positions to center the layers better\n        y_positions = np.linspace(0.9, 0.1, count)\n        xs = np.full(count, x_positions[layer_index])\n\n        # Draw the nodes\n        ax.scatter(xs, y_positions, s=500, color=c, edgecolor='black', zorder=3)\n        \n        # Store global position mapping\n        for i, (x, y) in enumerate(zip(xs, y_positions)):\n            global_to_pos[global_idx] = (x, y)\n            global_idx += 1\n\n    # 2. Draw connections\n    for post, pre in connections:\n        if pre in global_to_pos and post in global_to_pos:\n            xa, ya = global_to_pos[pre] \n            xb, yb = global_to_pos[post]\n            # Draw line from pre to post\n            ax.plot([xa, xb], [ya, yb], color='gray', linewidth=0.8, zorder=2)\n\n    # 3. Title and Axis\n    ax.set_title(title, fontsize=13)\n    ax.axis('off')\n\n    # 4. Add color legend\n    legend_labels = [\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=10, label=\"Entrada (Input)\"),\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[\"hidden\"], markersize=10, label=\"Capa Oculta (Hidden)\"),\n        Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[-1], markersize=10, label=\"Salida (Output)\")\n    ]\n    ax.legend(handles=legend_labels, loc='upper left', fontsize=10)\n\n# Plot experiment and save figures\n\ndef generate_figures(sys, x0, T, input_signal, title):\n    \"\"\"Runs simulation and generates 5 independent figures, saving them.\"\"\"\n    print(f\"Running simulation for {title}...\")\n    t, x, spikes = sys.simulate(x0, T, input_signal)\n    ctrl, obs = sys.controllability_observability()\n    print(f\"Linear Controllability: {ctrl}, Observability: {obs}\")\n\n    # --- 1) Arquitectura (Architecture) ---\n    fig1, ax1 = plt.subplots(figsize=(10, 6))\n    draw_architecture(ax1, sys.layers, sys.connections, f\"{title} - Arquitectura (Estructura: {sys.layers})\")\n    fig1.tight_layout()\n    plt.savefig('controllability_architecture.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: controllability_architecture.png\")\n    plt.close(fig1)\n\n    # --- 2) Entrada (Input) ---\n    fig2, ax2 = plt.subplots(figsize=(10, 3))\n    uvals = np.array([input_signal(ti) if callable(input_signal) else input_signal for ti in t])\n    \n    input_dim = sys.layers[0]\n    #flat case\n    if uvals.ndim == 1 and input_dim > 1:\n        uvals = uvals.reshape(-1, input_dim)\n    elif uvals.ndim == 1 and input_dim == 1:\n        # Single input, reshape to (T, 1) if necessary\n        uvals = uvals.reshape(-1, 1)\n\n    for i in range(input_dim):\n        ax2.plot(t, uvals[:, i], label=f\"Input {i+1}\", linestyle='--' if input_dim > 1 else '-')\n        \n    ax2.set_ylabel(\"Entrada\")\n    ax2.grid(True)\n    ax2.legend() \n    ax2.set_title(f\"Entrada | Linealmente Controlable: {ctrl}, Linealmente Observable: {obs}\")\n    ax2.set_xlabel(\"Tiempo (s)\")\n    fig2.tight_layout()\n    plt.savefig('controllability_input.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: controllability_input.png\")\n    plt.close(fig2)\n\n    # --- 3) Potenciales de neuronas ocultas ---\n    fig3, ax3 = plt.subplots(figsize=(10, 4))\n    hidden_start = sys.layers[0]\n    hidden_end = sys.n - sys.layers[-1]\n    \n    if hidden_end > hidden_start: \n        for idx in range(hidden_start, hidden_end):\n            # Using high-contrast color palette\n            color_index = idx - hidden_start\n            color = plt.cm.get_cmap('Set1')(color_index % 9)\n            ax3.plot(t, x[:, idx], label=f\"Oculta {idx - hidden_start + 1}\", color=color)\n        \n        ax3.axhline(sys.theta, color='gray', linestyle=':', label='Umbral')\n\n    else: \n        ax3.text(0.5, 0.5, \"No hay capa oculta para mostrar\", ha='center', va='center')\n        \n    ax3.set_ylabel(\"Potencial Oculto\")\n    ax3.set_title(\"Potenciales de la capa oculta\")\n    ax3.grid(True)\n    ax3.legend()\n    ax3.set_xlabel(\"Tiempo (s)\")\n    fig3.tight_layout()\n    plt.savefig('controllability_hidden.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: controllability_hidden.png\")\n    plt.close(fig3)\n\n    # --- 4) Membrana de salida + spikes (Output membrane + spikes) ---\n    fig4, ax4 = plt.subplots(figsize=(10, 4))\n    output_idx = sys.n - 1 # Assuming single output neuron\n    ax4.plot(t, x[:, output_idx], label=\"Membrana salida\", color='#2ECC71', linewidth=2)\n    ax4.axhline(sys.theta, color='#E74C3C', linestyle='--', label='Umbral')\n    \n    # Plot spikes slightly above the threshold line\n    ax4.eventplot(spikes[output_idx], lineoffsets=sys.theta + 0.05, colors='black', linelength=0.1)\n    ax4.set_ylabel(\"Potencial Salida\")\n    ax4.set_xlabel(\"Tiempo (s)\")\n    ax4.legend()\n    ax4.grid(True)\n    ax4.set_title(\"Membrana de salida y Umbral\")\n    fig4.tight_layout()\n    plt.savefig('controllability_output_membrane.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: controllability_output_membrane.png\")\n    plt.close(fig4)\n\n    # --- 5) Spikes de salida solamente (Output spikes only) ---\n    fig5, ax5 = plt.subplots(figsize=(10, 2))\n    ax5.eventplot(spikes[output_idx], lineoffsets=1, colors='k')\n    ax5.set_ylabel(\"Spike\")\n    ax5.set_xlabel(\"Tiempo (s)\")\n    ax5.set_title(\"Spikes de salida\")\n    ax5.set_yticks([])\n    ax5.grid(True)\n    fig5.tight_layout()\n    plt.savefig('controllability_spikes.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: controllability_spikes.png\")\n    plt.close(fig5)\n\ndef main():\n    # Use the architecture that was found to be controllable: [3, 1, 1]\n    LAYERS = [3, 1, 1]\n    \n    print(\"=== Generating Controllability Figures for [3, 1, 1] Architecture ===\")\n    \n    sys = LIFNetwork(LAYERS, tau=10.0, theta=1.0, reset=0.0, dt=0.001)\n    ctrl, obs = sys.controllability_observability()\n    \n    print(f\"Architecture {LAYERS} (n={sys.n}, L0={LAYERS[0]}): Controllable={ctrl}, Observable={obs}\")\n    \n    if ctrl:\n        T = 10.0  # Simulation time (seconds)\n        x0 = np.zeros(sys.n)\n        L0 = LAYERS[0]\n        # Input must match the input size L0. Set constant input of 1.5 for all L0 inputs.\n        u_input = lambda t: [1.5] * L0\n        \n        # Generate all figures\n        generate_figures(sys, x0, T, u_input, \n                        f\"Red Neuronal por Impulsos Controlable: {LAYERS[0]}→{LAYERS[1]}→{LAYERS[2]}\")\n        \n        print(\"\\n=== All figures generated successfully ===\")\n    else:\n        print(\"ERROR: Architecture is not controllable!\")\n\nif __name__ == \"__main__\":\n    main()","block_group":"ae333b8700f840b7a9a7e25e39ed61f8","execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"=== Generating Controllability Figures for [3, 1, 1] Architecture ===\nArchitecture [3, 1, 1] (n=5, L0=3): Controllable=True, Observable=True\nRunning simulation for Red Neuronal por Impulsos Controlable: 3→1→1...\nLinear Controllability: True, Observability: True\nSaved: controllability_architecture.png\nSaved: controllability_input.png\n"},{"name":"stderr","output_type":"stream","text":"C:\\Users\\tito\\AppData\\Local\\Temp\\ipykernel_62784\\179263517.py:214: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  color = plt.cm.get_cmap('Set1')(color_index % 9)\n"},{"name":"stdout","output_type":"stream","text":"Saved: controllability_hidden.png\nSaved: controllability_output_membrane.png\nSaved: controllability_spikes.png\n\n=== All figures generated successfully ===\n"}],"outputs_reference":"dbtable:cell_outputs/c25cc5ef-8c5e-4d37-9070-061d1aeaeba3","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c7d4307c-0ebe-4a0e-91ec-84ab60ddfdb3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"d2f18dcbe7b74dee9999afdac474824f"}}