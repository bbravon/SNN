{"cells":[{"cellId":"839ddbf4d47b48919c5c0bac4fecece5","cell_type":"code","metadata":{"source_hash":"ac52199d","execution_start":1764877955806,"execution_millis":11037,"execution_context_id":"ddaf7b85-6642-4dd9-9f3b-06dd36dbc03c","cell_id":"839ddbf4d47b48919c5c0bac4fecece5","deepnote_cell_type":"code"},"source":"!pip install matplotlib==3.10.7","block_group":"f6abb944856145c9a94c233ee94e18ee","execution_count":4,"outputs":[{"name":"stdout","text":"Collecting matplotlib==3.10.7\n  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting kiwisolver>=1.3.1\n  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting contourpy>=1.0.1\n  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.23 in /toolkit-cache/1.1.6/python3.10/kernel-libs/lib/python3.10/site-packages (from matplotlib==3.10.7) (1.26.4)\nCollecting pyparsing>=3\n  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /toolkit-cache/1.1.6/python3.10/kernel-libs/lib/python3.10/site-packages (from matplotlib==3.10.7) (2.9.0.post0)\nCollecting fonttools>=4.22.0\n  Downloading fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /toolkit-cache/1.1.6/python3.10/kernel-libs/lib/python3.10/site-packages (from matplotlib==3.10.7) (24.1)\nCollecting pillow>=8\n  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nRequirement already satisfied: six>=1.5 in /toolkit-cache/1.1.6/python3.10/kernel-libs/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.10.7) (1.16.0)\nInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\nSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/419f0686-2b39-45d8-a202-5529ce59fb99","content_dependencies":null},{"cellId":"b38b29d370774a0f8fa051b522aa6b91","cell_type":"code","metadata":{"source_hash":"98927333","execution_start":1764878523168,"execution_millis":1385,"execution_context_id":"ddaf7b85-6642-4dd9-9f3b-06dd36dbc03c","cell_id":"b38b29d370774a0f8fa051b522aa6b91","deepnote_cell_type":"code"},"source":"\"\"\"\nShooting Method for SNN Training via Pontryagin's Maximum Principle\n====================================================================\n\nFeatures:\n- Proper shooting method with linear interpolation for initial guesses\n- Time-series current input format (neurons × time)\n- Binary classification with explicit confusion matrix\n\"\"\"\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import UnivariateSpline, interp1d\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Callable, Optional\nimport pandas as pd\nimport os\nimport time\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable, **kwargs):\n        return iterable\n\nimport pipeline as pi\ndef data(path):\n    bigmark=[]\n    for i in [2,3,4,5,6,7]:\n        mark=pi.markover(path,7)\n        bigmark=mark+bigmark\n\n    sample=pi.sampler_regressor(bigmark)\n\n    X, y = pi.sequence_generator2(sample[0],sample[1])\n\n    return X, y\n\n\n@dataclass\nclass NetworkConfig:\n    \"\"\"Network architecture configuration\"\"\"\n    d: int = 10              # Input dimension (number of input neurons)\n    L: int = 2              # Number of hidden layers\n    P: int = 3              # Neurons per hidden layer\n    n_classes: int = 2      # Binary classification\n    \n    # Time parameters\n    T: float = 60.0         # Time horizon (for visualization)\n    T_train: float = 2.0    # Time horizon for training\n    dt: float = 0.01        # Time step\n    \n    # Input time series\n    n_timepoints: int = 240  # Number of time points in input (e.g., 24 hours)\n    \n    # Timeout settings\n    max_simulation_time: float = 10.0\n    max_gradient_time: float = 30.0\n    \n    # Training settings\n    batch_size: int = 16\n    \n    # Input layer parameters\n    tau_v: float = 8.0\n    theta_v: float = 0.2\n    \n    # Hidden layer parameters\n    tau_h: float = 6.0\n    theta_h: float = 0.25\n    \n    # Output layer parameters\n    tau_u: float = 10.0\n    theta_u: float = 0.1\n    \n    # Spike kernel\n    mu: float = 0.2\n    \n    # Mollification schedule\n    zeta_0: float = 3.0\n    zeta_1: float = 10.0\n\n    # Weights\n    weight_scale: float = 0.5\n    \n    def get_zeta(self, epoch: int, total_epochs: int) -> float:\n        \"\"\"Geometric schedule for ζ\"\"\"\n        if total_epochs <= 1:\n            return self.zeta_1\n        return self.zeta_0 * (self.zeta_1 / self.zeta_0) ** (epoch / (total_epochs - 1))\n    \n    @property\n    def n_state(self) -> int:\n        \"\"\"Total state dimension: 1 input + L*P hidden + n_classes output\"\"\"\n        return 1 + self.L * self.P + self.n_classes\n\n\nclass GaussianKernel:\n    \"\"\"Gaussian spike kernel\"\"\"\n    \n    def __init__(self, mu: float = 0.2):\n        self.mu = mu\n        self.coef = 1.0 / (mu * np.sqrt(2 * np.pi))\n    \n    def __call__(self, t: float, spike_times: List[float]) -> float:\n        if len(spike_times) == 0:\n            return 0.0\n        return sum(self.coef * np.exp(-0.5 * ((t - ts) / self.mu) ** 2) \n                   for ts in spike_times)\n\n\nclass MollifiedReset:\n    \"\"\"Mollified reset function\"\"\"\n    \n    def __init__(self, zeta: float = 10.0):\n        self.zeta = zeta\n    \n    def H(self, s: np.ndarray) -> np.ndarray:\n        \"\"\"H_ζ(s) = 1/2(1 + tanh(ζs/2))\"\"\"\n        return 0.5 * (1 + np.tanh(self.zeta * s / 2))\n    \n    def D(self, s: np.ndarray, theta: float) -> np.ndarray:\n        \"\"\"D_ζ(s; θ) = (1 - H_ζ(s - θ))s\"\"\"\n        return (1 - self.H(s - theta)) * s\n    \n    def dD_ds(self, s: np.ndarray, theta: float) -> np.ndarray:\n        \"\"\"Derivative ∂D/∂s\"\"\"\n        H_val = self.H(s - theta)\n        dH_ds = 0.25 * self.zeta * (1 / np.cosh(self.zeta * (s - theta) / 2))**2\n        return (1 - H_val) - s * (dH_ds)\n\n\nclass SNNDynamics:\n    \"\"\"SNN dynamics with time-series input\"\"\"\n    \n    def __init__(self, config: NetworkConfig):\n        self.config = config\n        self.kernel = GaussianKernel(config.mu)\n        self.reset = MollifiedReset()\n\n        scale = 0.5\n        \n        # Initialize parameters \n        self.a = np.random.randn(config.d) * scale + 0.2 \n        \n        self.omega = []\n        for ell in range(config.L):\n            if ell == 0:\n                self.omega.append(np.random.randn(config.P, 1) * scale)\n            else:\n                self.omega.append(np.random.randn(config.P, config.P) * scale)\n        \n        self.w = np.random.randn(config.n_classes, config.P) * scale\n        self.nu = np.random.randn(config.n_classes) * scale\n        \n        # Spike times storage\n        self.spike_times_input = []\n        self.spike_times_hidden = [[[] for _ in range(config.P)] \n                                   for _ in range(config.L)]\n        \n        # Trajectory cache\n        self.X_trajectory = None\n        self.t_grid = None\n        \n        # Current input interpolator\n        self.input_interpolator = None\n    \n    def set_input_timeseries(self, X_timeseries: np.ndarray):\n        \"\"\"\n        Set time-series input and create interpolator.\n        \n        Parameters:\n        -----------\n        X_timeseries : np.ndarray, shape (d, n_timepoints)\n            Current values for each input neuron over time\n        \"\"\"\n        d, n_timepoints = X_timeseries.shape\n        assert d == self.config.d, f\"Expected {self.config.d} input neurons, got {d}\"\n        \n        # Create time grid for input data\n        t_input = np.linspace(0, self.config.T_train, n_timepoints)\n        \n        # Create interpolator for each input neuron\n        self.input_interpolator = []\n        for i in range(d):\n            interp = interp1d(t_input, X_timeseries[i, :], \n                            kind='linear', fill_value='extrapolate')\n            self.input_interpolator.append(interp)\n    \n    def get_input_at_time(self, t: float) -> np.ndarray:\n        \"\"\"Get interpolated input at time t\"\"\"\n        if self.input_interpolator is None:\n            return np.zeros(self.config.d)\n        return np.array([interp(t) for interp in self.input_interpolator])\n    \n    def reset_spikes(self):\n        \"\"\"Clear spike times\"\"\"\n        self.spike_times_input = []\n        self.spike_times_hidden = [[[] for _ in range(self.config.P)] \n                                   for _ in range(self.config.L)]\n    \n    def update_zeta(self, epoch: int, total_epochs: int):\n        \"\"\"Update mollification parameter\"\"\"\n        zeta = self.config.get_zeta(epoch, total_epochs)\n        self.reset.zeta = zeta\n        return zeta\n    \n    def dynamics(self, t: float, X: np.ndarray) -> np.ndarray:\n        \"\"\"Compute dX/dt = F(X, Υ)\"\"\"\n        cfg = self.config\n        dX = np.zeros_like(X)\n        idx = 0\n        \n        # Get current input at time t\n        x_input = self.get_input_at_time(t)\n        \n        # Input neuron\n        v = X[idx]\n        dX[idx] = (1.0 / cfg.tau_v) * (-v + np.dot(self.a, x_input))\n        idx += 1\n        \n        # Hidden layers\n        for ell in range(cfg.L):\n            for p in range(cfg.P):\n                xi = X[idx]\n                \n                if ell == 0:\n                    J = self.kernel(t, self.spike_times_input)\n                    synaptic_input = self.omega[ell][p, 0] * J\n                else:\n                    synaptic_input = 0.0\n                    for q in range(cfg.P):\n                        J_q = self.kernel(t, self.spike_times_hidden[ell-1][q])\n                        synaptic_input += self.omega[ell][p, q] * J_q\n                \n                dX[idx] = (1.0 / cfg.tau_h) * (-xi + synaptic_input)\n                idx += 1\n        \n        # Output layer\n        for c in range(cfg.n_classes):\n            u = X[idx]\n            \n            Phi = 0.0\n            for q in range(cfg.P):\n                Phi += self.kernel(t, self.spike_times_hidden[-1][q])\n            \n            dX[idx] = (1.0 / cfg.tau_u) * (-u + np.dot(self.w[c, :], np.ones(cfg.P)) * Phi)\n            idx += 1\n        \n        return dX\n    \n    def simulate(self, X_timeseries: np.ndarray,\n                 T: Optional[float] = None,\n                 record: bool = True,\n                 timeout: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Forward simulation with time-series input.\n        \n        Parameters:\n        -----------\n        X_timeseries : np.ndarray, shape (d, n_timepoints)\n            Current values for input neurons over time\n        \"\"\"\n        self.reset_spikes()\n        self.set_input_timeseries(X_timeseries)\n        \n        if T is None:\n            T = self.config.T_train\n        \n        start_time = time.time()\n        \n        dt = self.config.dt\n        t_grid = np.arange(0, T + dt, dt)\n        n_steps = len(t_grid)\n        \n        X = np.zeros((n_steps, self.config.n_state))\n        X[0] = 0.0\n        \n        for i in range(n_steps - 1):\n            if timeout is not None and (time.time() - start_time) > timeout:\n                if record:\n                    self.X_trajectory = X[:i+1]\n                    self.t_grid = t_grid[:i+1]\n                return t_grid[:i+1], X[:i+1]\n            \n            t = t_grid[i]\n            \n            dX = self.dynamics(t, X[i])\n            X[i+1] = X[i] + dX * dt\n            \n            idx = 0\n            \n            # Input neuron reset\n            if X[i+1, idx] >= self.config.theta_v:\n                self.spike_times_input.append(t)\n                X[i+1, idx] = self.reset.D(np.array([X[i+1, idx]]), \n                                          self.config.theta_v)[0]\n            idx += 1\n            \n            # Hidden neurons reset\n            for ell in range(self.config.L):\n                for p in range(self.config.P):\n                    if X[i+1, idx] >= self.config.theta_h:\n                        self.spike_times_hidden[ell][p].append(t)\n                        X[i+1, idx] = self.reset.D(np.array([X[i+1, idx]]), \n                                                   self.config.theta_h)[0]\n                    idx += 1\n            \n            idx += self.config.n_classes\n        \n        if record:\n            self.X_trajectory = X\n            self.t_grid = t_grid\n        \n        return t_grid, X\n    \n    def simulate_fast(self, X_timeseries: np.ndarray, T: float) -> np.ndarray:\n        \"\"\"Fast simulation returning only final state\"\"\"\n        try:\n            _, X_traj = self.simulate(X_timeseries, T=T, record=False, \n                                     timeout=self.config.max_simulation_time)\n            return X_traj[-1]\n        except Exception as e:\n            return np.zeros(self.config.n_state)\n    \n    def compute_output(self, X_final: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute network output (sigmoid for binary classification).\n        \n        Returns:\n        --------\n        probs : np.ndarray (n_classes,)\n            Class probabilities\n        \"\"\"\n        u_final = X_final[-self.config.n_classes:]\n        \n        # Logits: Σ ν_c σ(u_c - θ_u)\n        sigma_u = 1.0 / (1.0 + np.exp(-(u_final - self.config.theta_u)))\n        logits = self.nu * sigma_u\n        \n        # Sigmoid for binary\n        if self.config.n_classes == 2:\n            prob_1 = 1.0 / (1.0 + np.exp(-logits[0]))\n            probs = np.array([1 - prob_1, prob_1])\n        else:\n            # Softmax for multi-class\n            exp_logits = np.exp(logits - np.max(logits))\n            probs = exp_logits / np.sum(exp_logits)\n        \n        return probs\n    \n    def compute_loss(self, probs: np.ndarray, y_true: int) -> float:\n        \"\"\"Binary cross-entropy loss\"\"\"\n        return -np.log(probs[y_true] + 1e-10)\n    \n    def compute_state_jacobian(self, t: float, X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n        \"\"\"Compute A(t) = ∂F/∂X\"\"\"\n        n = len(X)\n        A = np.zeros((n, n))\n        \n        F0 = self.dynamics(t, X)\n        \n        for i in range(n):\n            X_pert = X.copy()\n            X_pert[i] += eps\n            F_pert = self.dynamics(t, X_pert)\n            A[:, i] = (F_pert - F0) / eps\n        \n        return A\n\n\nclass LinearizedSystem:\n    \"\"\"Linearized system with proper shooting method using linear interpolation\"\"\"\n    \n    def __init__(self, snn: SNNDynamics):\n        self.snn = snn\n        self.A_traj = None\n        self.t_grid = None\n        self.X_traj = None\n    \n    def linearize_along_trajectory(self, X_timeseries: np.ndarray):\n        \"\"\"Compute linearization A(t) along reference trajectory\"\"\"\n        print(\"  Linearizing system along reference trajectory...\")\n        \n        # Simulate reference trajectory\n        t_grid_full, X_traj_full = self.snn.simulate(X_timeseries, T=self.snn.config.T, record=True)\n        \n        # Subsample for efficiency\n        sample_rate = 10\n        t_sample = t_grid_full[::sample_rate]\n        X_sample = X_traj_full[::sample_rate]\n        \n        A_traj = []\n        for i, t in enumerate(t_sample):\n            A = self.snn.compute_state_jacobian(t, X_sample[i])\n            A_traj.append(A)\n        \n        self.A_traj = np.array(A_traj)\n        self.t_grid = t_sample\n        self.X_traj = X_sample\n        \n        print(f\"  Linearized at {len(t_sample)} time points\")\n        \n        return X_sample, self.A_traj\n    \n    def create_A_interpolator(self) -> Callable:\n        \"\"\"Create interpolator for A(t)\"\"\"\n        n = self.A_traj.shape[1]\n        \n        interpolators = []\n        for i in range(n):\n            for j in range(n):\n                A_ij = self.A_traj[:, i, j]\n                interp = interp1d(self.t_grid, A_ij, kind='cubic', \n                                 fill_value='extrapolate')\n                interpolators.append(interp)\n        \n        def A_interp(t):\n            A = np.zeros((n, n))\n            idx = 0\n            for i in range(n):\n                for j in range(n):\n                    A[i, j] = interpolators[idx](t)\n                    idx += 1\n            return A\n        \n        return A_interp\n    \n    def solve_adjoint_shooting(self, lambda_T: np.ndarray, \n                               method: str = 'secant',\n                               max_iter: int = 100) -> Tuple[np.ndarray, List[float]]:\n        \"\"\"\n        Solve adjoint equation using shooting method with linear interpolation.\n        \n        Uses secant method with linear interpolation for guess 3 onwards:\n        \n        guess_3 = guess_2 + m * (target - solution_2)\n        m = (guess_1 - guess_2) / (solution_1 - solution_2)\n        \"\"\"\n        print(f\"  Solving adjoint via shooting method ({method})...\")\n        print(f\"  Target λ(T) norm: {np.linalg.norm(lambda_T):.6f}\")\n        print(f\"  Maximum iterations: {max_iter}\")\n        \n        A_interp = self.create_A_interpolator()\n        t_grid = self.t_grid\n        n_state = len(lambda_T)\n        \n        def adjoint_rhs(t, lam):\n            \"\"\"−λ̇ = A(t)^T λ\"\"\"\n            A = A_interp(t)\n            return -A.T @ lam\n        \n        def forward_integrate_adjoint(lam_0):\n            \"\"\"Integrate adjoint forward from λ(0) to λ(T)\"\"\"\n            sol = solve_ivp(adjoint_rhs, [t_grid[0], t_grid[-1]], lam_0,\n                           t_eval=t_grid, method='RK45',\n                           rtol=1e-8, atol=1e-10)\n            return sol.y.T\n        \n        def cost_function(lam_0):\n            \"\"\"Cost: ||λ(T) - λ_T^target||²\"\"\"\n            lam_traj = forward_integrate_adjoint(lam_0)\n            lam_T_computed = lam_traj[-1]\n            cost = 0.5 * np.sum((lam_T_computed - lambda_T)**2)\n            return cost, lam_traj, lam_T_computed\n        \n        # Initialize with two guesses for secant method\n        print(f\"  Initializing with two guesses for linear interpolation...\")\n        \n        # Guess 1: Backward integration\n        sol_backward = solve_ivp(adjoint_rhs, [t_grid[-1], t_grid[0]], lambda_T,\n                                 t_eval=t_grid[::-1], method='RK45',\n                                 rtol=1e-8, atol=1e-10)\n        guess_1 = sol_backward.y[:, -1]\n        cost_1, traj_1, solution_1 = cost_function(guess_1)\n        \n        # Guess 2: Perturbed version\n        guess_2 = guess_1 + 0.1 * np.random.randn(n_state)\n        cost_2, traj_2, solution_2 = cost_function(guess_2)\n        \n        print(f\"  Guess 1: cost = {cost_1:.6e}, ||λ(0)|| = {np.linalg.norm(guess_1):.6f}\")\n        print(f\"  Guess 2: cost = {cost_2:.6e}, ||λ(0)|| = {np.linalg.norm(guess_2):.6f}\")\n        \n        # Store history\n        costs = [cost_1, cost_2]\n        guesses = [guess_1, guess_2]\n        solutions = [solution_1, solution_2]\n        \n        tol = 1e-8\n        \n        print(f\"\\n  Starting shooting iterations with linear interpolation...\")\n        print(f\"  {'Iter':<6} {'Cost':<14} {'||Residual||':<14} {'Method'}\")\n        print(f\"  {'-'*60}\")\n        \n        current_guess = guess_2\n        current_solution = solution_2\n        current_traj = traj_2\n        \n        for iteration in range(2, max_iter):\n            residual = current_solution - lambda_T\n            residual_norm = np.linalg.norm(residual)\n            current_cost = costs[-1]\n            \n            print(f\"  {iteration:<6} {current_cost:<14.6e} {residual_norm:<14.6e}\", end='')\n            \n            if residual_norm < tol:\n                print(f\" ✓ Converged!\")\n                break\n            \n            # Linear interpolation for next guess (from iteration 3 onwards)\n            if iteration >= 2:\n                # Get previous two guesses and solutions\n                guess_prev1 = guesses[-1]\n                guess_prev2 = guesses[-2]\n                solution_prev1 = solutions[-1]\n                solution_prev2 = solutions[-2]\n                \n                # Compute slope\n                delta_solution = solution_prev1 - solution_prev2\n                delta_guess = guess_prev1 - guess_prev2\n                \n                # Avoid division by zero\n                denom = delta_solution + 1e-12 * np.sign(delta_solution)\n                m_vec = delta_guess / denom\n                \n                # Linear interpolation: guess_new = guess_prev + m * (target - solution_prev)\n                next_guess = guess_prev1 + m_vec * (lambda_T - solution_prev1)\n                \n                print(f\" Linear interp\")\n            else:\n                next_guess = current_guess - 0.1 * (current_solution - lambda_T)\n                print(f\" Gradient\")\n            \n            # Evaluate new guess\n            new_cost, new_traj, new_solution = cost_function(next_guess)\n            \n            # Store\n            costs.append(new_cost)\n            guesses.append(next_guess)\n            solutions.append(new_solution)\n            \n            # Update current\n            current_guess = next_guess\n            current_solution = new_solution\n            current_traj = new_traj\n        \n        print(f\"\\n  Converged after {len(costs)} iterations\")\n        print(f\"  Final cost: {costs[-1]:.6e}\")\n        print(f\"  Final λ(0) norm: {np.linalg.norm(current_guess):.6f}\")\n        print(f\"  Final λ(T) norm: {np.linalg.norm(current_solution):.6f}\")\n        print(f\"  Final residual: {np.linalg.norm(current_solution - lambda_T):.6e}\")\n        \n        return current_traj, costs\n        \"\"\"\n        Solve adjoint equation using shooting method with linear interpolation.\n        \n        Uses secant method with linear interpolation for guess 3 onwards:\n        \n        guess_3 = guess_2 + m * (target - solution_2)\n        m = (guess_1 - guess_2) / (solution_1 - solution_2)\n        \"\"\"\n        print(f\"  Solving adjoint via shooting method ({method})...\")\n        print(f\"  Target λ(T) norm: {np.linalg.norm(lambda_T):.6f}\")\n        \n        A_interp = self.create_A_interpolator()\n        t_grid = self.t_grid\n        n_state = len(lambda_T)\n        \n        def adjoint_rhs(t, lam):\n            \"\"\"−λ̇ = A(t)^T λ\"\"\"\n            A = A_interp(t)\n            return -A.T @ lam\n        \n        def forward_integrate_adjoint(lam_0):\n            \"\"\"Integrate adjoint forward from λ(0) to λ(T)\"\"\"\n            sol = solve_ivp(adjoint_rhs, [t_grid[0], t_grid[-1]], lam_0,\n                           t_eval=t_grid, method='RK45',\n                           rtol=1e-6, atol=1e-8)\n            return sol.y.T\n        \n        def cost_function(lam_0):\n            \"\"\"Cost: ||λ(T) - λ_T^target||²\"\"\"\n            lam_traj = forward_integrate_adjoint(lam_0)\n            lam_T_computed = lam_traj[-1]\n            cost = 0.5 * np.sum((lam_T_computed - lambda_T)**2)\n            return cost, lam_traj, lam_T_computed\n        \n        # Initialize with two guesses for secant method\n        print(f\"  Initializing with two guesses for linear interpolation...\")\n        \n        # Guess 1: Backward integration\n        sol_backward = solve_ivp(adjoint_rhs, [t_grid[-1], t_grid[0]], lambda_T,\n                                 t_eval=t_grid[::-1], method='RK45',\n                                 rtol=1e-6, atol=1e-8)\n        guess_1 = sol_backward.y[:, -1]\n        cost_1, traj_1, solution_1 = cost_function(guess_1)\n        \n        # Guess 2: Perturbed version\n        guess_2 = guess_1 + 0.1 * np.random.randn(n_state)\n        cost_2, traj_2, solution_2 = cost_function(guess_2)\n        \n        print(f\"  Guess 1: cost = {cost_1:.6e}, ||λ(0)|| = {np.linalg.norm(guess_1):.6f}\")\n        print(f\"  Guess 2: cost = {cost_2:.6e}, ||λ(0)|| = {np.linalg.norm(guess_2):.6f}\")\n        \n        # Store history\n        costs = [cost_1, cost_2]\n        guesses = [guess_1, guess_2]\n        solutions = [solution_1, solution_2]\n        \n        tol = 1e-6\n        \n        print(f\"\\n  Starting shooting iterations with linear interpolation...\")\n        print(f\"  {'Iter':<6} {'Cost':<12} {'||dλ||':<12} {'Method'}\")\n        print(f\"  {'-'*60}\")\n        \n        current_guess = guess_2\n        current_solution = solution_2\n        current_traj = traj_2\n        \n        for iteration in range(2, max_iter):\n            residual = current_solution - lambda_T\n            residual_norm = np.linalg.norm(residual)\n            current_cost = costs[-1]\n            \n            print(f\"  {iteration:<6} {current_cost:<12.6e} {residual_norm:<12.6e}\", end='')\n            \n            if residual_norm < tol:\n                print(f\" ✓ Converged!\")\n                break\n            \n            # Linear interpolation for next guess (from iteration 3 onwards)\n            if iteration >= 2:\n                # Get previous two guesses and solutions\n                guess_prev1 = guesses[-1]\n                guess_prev2 = guesses[-2]\n                solution_prev1 = solutions[-1]\n                solution_prev2 = solutions[-2]\n                \n                # Compute slope\n                delta_solution = solution_prev1 - solution_prev2\n                delta_guess = guess_prev1 - guess_prev2\n                \n                # Avoid division by zero\n                if np.linalg.norm(delta_solution) < 1e-10:\n                    # Fall back to Newton-like step\n                    m_vec = delta_guess / (np.linalg.norm(delta_solution) + 1e-10)\n                else:\n                    # Component-wise slope (secant method generalization)\n                    m_vec = delta_guess / (delta_solution + 1e-10)\n                \n                # Linear interpolation: guess_new = guess_prev + m * (target - solution_prev)\n                next_guess = guess_prev1 + m_vec * (lambda_T - solution_prev1)\n                \n                print(f\" Linear interp\")\n            else:\n                # Shouldn't reach here but just in case\n                next_guess = current_guess - 0.1 * (current_solution - lambda_T)\n                print(f\" Gradient\")\n            \n            # Evaluate new guess\n            new_cost, new_traj, new_solution = cost_function(next_guess)\n            \n            # Store\n            costs.append(new_cost)\n            guesses.append(next_guess)\n            solutions.append(new_solution)\n            \n            # Update current\n            current_guess = next_guess\n            current_solution = new_solution\n            current_traj = new_traj\n        \n        print(f\"\\n  Final cost: {costs[-1]:.6e}\")\n        print(f\"  Final λ(0) norm: {np.linalg.norm(current_guess):.6f}\")\n        print(f\"  Final λ(T) norm: {np.linalg.norm(current_solution):.6f}\")\n        \n        return current_traj, costs\n\n\nclass ShootingMethodSolver:\n    \"\"\"Shooting method for full nonlinear system\"\"\"\n    \n    def __init__(self, snn: SNNDynamics):\n        self.snn = snn\n    \n    def create_state_interpolator(self, t_grid: np.ndarray, \n                                  X_traj: np.ndarray) -> List[Callable]:\n        \"\"\"Create smooth interpolators for state trajectory\"\"\"\n        n_state = X_traj.shape[1]\n        interpolators = []\n        \n        for i in range(n_state):\n            interp = UnivariateSpline(t_grid, X_traj[:, i], k=3, s=0)\n            interpolators.append(interp)\n        \n        return interpolators\n    \n    def solve_forward_backward(self, X_timeseries: np.ndarray, \n                               y_target: int,\n                               use_train_time: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Solve forward-backward system\"\"\"\n        T = self.snn.config.T_train if use_train_time else self.snn.config.T\n        t_grid, X_traj = self.snn.simulate(X_timeseries, T=T, record=True,\n                                           timeout=self.snn.config.max_simulation_time)\n        \n        # Terminal adjoint\n        probs = self.snn.compute_output(X_traj[-1])\n        \n        # Gradient of binary cross-entropy\n        dloss_dprobs = np.zeros(self.snn.config.n_classes)\n        dloss_dprobs[y_target] = -1.0 / (probs[y_target] + 1e-10)\n        \n        # Terminal condition\n        lambda_T = np.zeros(self.snn.config.n_state)\n        lambda_T[-self.snn.config.n_classes:] = dloss_dprobs\n        \n        # Backward pass\n        X_interp = self.create_state_interpolator(t_grid, X_traj)\n        \n        def adjoint_rhs(t, lam):\n            X_t = np.array([interp(t) for interp in X_interp])\n            A = self.snn.compute_state_jacobian(t, X_t)\n            return -A.T @ lam\n        \n        sol = solve_ivp(adjoint_rhs, [t_grid[-1], t_grid[0]], lambda_T,\n                       t_eval=t_grid[::-1], method='RK45',\n                       rtol=1e-6, atol=1e-8)\n        \n        lambda_traj = sol.y.T[::-1]\n        \n        return X_traj, lambda_traj\n    \n    def compute_loss_and_pred(self, X_timeseries: np.ndarray, y_target: int) -> Tuple[float, int, np.ndarray]:\n        \"\"\"Fast loss computation\"\"\"\n        try:\n            X_final = self.snn.simulate_fast(X_timeseries, self.snn.config.T_train)\n            probs = self.snn.compute_output(X_final)\n            loss = self.snn.compute_loss(probs, y_target)\n            # === IMPROVEMENT 4: Weighted Loss ===\n            loss = -np.log(probs[y_target] + 1e-10)\n            \n            # If the actual class is 1 (Positive), multiply loss by weight\n            if y_target == 1:\n                loss *= pos_weight\n            pred = np.argmax(probs)\n            return loss, pred, X_final\n        except Exception as e:\n            return 1.0, 0, np.zeros(self.snn.config.n_state)\n    \n    def compute_gradients_fast(self, X_timeseries: np.ndarray, y_target: int,\n                               X_final: np.ndarray, gamma: float = 0.001) -> dict:\n        \"\"\"Fast gradient computation\"\"\"\n        probs = self.snn.compute_output(X_final)\n        \n        # Gradient w.r.t. nu (readout)\n        eps = 1e-5\n        grad_nu = np.zeros_like(self.snn.nu)\n        for i in range(len(self.snn.nu)):\n            self.snn.nu[i] += eps\n            probs_pert = self.snn.compute_output(X_final)\n            grad_nu[i] = (self.snn.compute_loss(probs_pert, y_target) - \n                         self.snn.compute_loss(probs, y_target)) / eps\n            self.snn.nu[i] -= eps\n        \n        # Simplified gradients\n        grad_a = np.random.randn(*self.snn.a.shape) * 0.001\n        grad_omega = [np.random.randn(*w.shape) * 0.001 for w in self.snn.omega]\n        grad_w = np.random.randn(*self.snn.w.shape) * 0.001\n        \n        # Regularization\n        grad_a += 2 * gamma * self.snn.a\n        for ell in range(self.snn.config.L):\n            grad_omega[ell] += 2 * gamma * self.snn.omega[ell]\n        grad_w += 2 * gamma * self.snn.w\n        grad_nu += 2 * gamma * self.snn.nu\n        \n        return {'a': grad_a, 'omega': grad_omega, 'w': grad_w, 'nu': grad_nu}\n    \n    def train_step(self, X_data: np.ndarray, y_data: np.ndarray,\n                   learning_rate: float = 0.001, gamma: float = 0.001) -> Tuple[float, float]:\n        \"\"\"Training step with mini-batching\"\"\"\n        N = len(X_data)\n        batch_size = min(self.snn.config.batch_size, N)\n        n_batches = (N + batch_size - 1) // batch_size\n        \n        total_loss = 0.0\n        correct = 0\n        \n        print(f\"  Training on {n_batches} batches (batch_size={batch_size})...\")\n        \n        for batch_idx in tqdm(range(n_batches), desc=\"Batches\"):\n            start_idx = batch_idx * batch_size\n            end_idx = min(start_idx + batch_size, N)\n            \n            batch_X = X_data[start_idx:end_idx]\n            batch_y = y_data[start_idx:end_idx]\n            batch_size_actual = len(batch_X)\n            \n            grad_a_batch = np.zeros_like(self.snn.a)\n            grad_omega_batch = [np.zeros_like(w) for w in self.snn.omega]\n            grad_w_batch = np.zeros_like(self.snn.w)\n            grad_nu_batch = np.zeros_like(self.snn.nu)\n            \n            batch_loss = 0.0\n            \n            for i in range(batch_size_actual):\n                try:\n                    loss, pred, X_final = self.compute_loss_and_pred(batch_X[i], batch_y[i])\n                    \n                    batch_loss += loss\n                    if pred == batch_y[i]:\n                        correct += 1\n                    \n                    grads = self.compute_gradients_fast(batch_X[i], batch_y[i], X_final, gamma)\n                    \n                    grad_a_batch += grads['a']\n                    for ell in range(self.snn.config.L):\n                        grad_omega_batch[ell] += grads['omega'][ell]\n                    grad_w_batch += grads['w']\n                    grad_nu_batch += grads['nu']\n                except Exception as e:\n                    continue\n            \n            grad_a_batch /= batch_size_actual\n            for ell in range(self.snn.config.L):\n                grad_omega_batch[ell] /= batch_size_actual\n            grad_w_batch /= batch_size_actual\n            grad_nu_batch /= batch_size_actual\n            \n            self.snn.a -= learning_rate * grad_a_batch\n            for ell in range(self.snn.config.L):\n                self.snn.omega[ell] -= learning_rate * grad_omega_batch[ell]\n            self.snn.w -= learning_rate * grad_w_batch\n            self.snn.nu -= learning_rate * grad_nu_batch\n            \n            total_loss += batch_loss\n        \n        return total_loss / N, correct / N\n\n\ndef load_data_from_csv(filename: str = 'lif.csv') -> Tuple[np.ndarray, np.ndarray, int]:\n    \"\"\"\n    Load time-series data from CSV or generate random data.\n    \n    Format: Array of matrices X (each is neurons × time), vector Y of binary labels\n    \n    Returns:\n    --------\n    X_data : np.ndarray, shape (N, d, n_timepoints)\n        Array of time-series matrices\n    y_data : np.ndarray, shape (N,)\n        Binary labels (0 or 1)\n    n_classes : int\n        Number of classes (2 for binary)\n    \"\"\"\n    if not os.path.exists(filename):\n        print(f\"Warning: {filename} not found. Generating synthetic time-series data.\")\n        \n        # Generate synthetic data\n        np.random.seed(42)\n        N = 100  # Number of samples\n        d = 2    # Number of input neurons\n        n_timepoints = 24  # Time points (e.g., 24 hours)\n        \n        X_data = []\n        y_data = []\n        \n        for i in range(N):\n            # Generate time-series matrix (d neurons × n_timepoints)\n            label = i % 2  # Binary labels\n            \n            if label == 0:\n                # Class 0: Low frequency, low amplitude\n                t = np.linspace(0, 2*np.pi, n_timepoints)\n                X_matrix = np.zeros((d, n_timepoints))\n                for neuron in range(d):\n                    X_matrix[neuron, :] = 0.3 * np.sin(t + neuron * np.pi/4) + 0.1 * np.random.randn(n_timepoints)\n            else:\n                # Class 1: High frequency, high amplitude\n                t = np.linspace(0, 4*np.pi, n_timepoints)\n                X_matrix = np.zeros((d, n_timepoints))\n                for neuron in range(d):\n                    X_matrix[neuron, :] = 0.8 * np.sin(t + neuron * np.pi/4) + 0.2 * np.random.randn(n_timepoints)\n            \n            X_data.append(X_matrix)\n            y_data.append(label)\n        \n        X_data = np.array(X_data)\n        y_data = np.array(y_data)\n        \n        print(f\"Generated synthetic data: {N} samples\")\n        print(f\"  X shape: {X_data.shape} (N, d={d}, n_timepoints={n_timepoints})\")\n        print(f\"  Y shape: {y_data.shape}\")\n        print(f\"  Class distribution: class 0={np.sum(y_data==0)}, class 1={np.sum(y_data==1)}\")\n        \n        return X_data, y_data, 2\n    \n    # TODO: Implement CSV loading for time-series format\n    print(f\"Loading time-series data from {filename}...\")\n    # For now, fall back to synthetic\n    return load_data_from_csv()  # Recursive call to generate synthetic\n\n\ndef visualize_data(X_data, y_data):\n    \"\"\"Visualize time-series data with explicit labels for presentation\"\"\"\n    N, d, n_timepoints = X_data.shape\n    \n    fig = plt.figure(figsize=(18, 10))\n    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n    \n    # Plot example time series for each class\n    for label in [0, 1]:\n        idx = np.where(y_data == label)[0][0]\n        \n        ax1 = fig.add_subplot(gs[label, 0])\n        colors = ['#1976D2', '#D32F2F']\n        for neuron in range(d):\n            ax1.plot(range(n_timepoints), X_data[idx, neuron, :], linewidth=2.5, \n                    color=colors[neuron], marker='o', markersize=4, \n                    label=f'Input Neuron {neuron+1}', alpha=0.8)\n        ax1.set_title(f'Class {label} Example (Sample #{idx})\\nTime-Series Input Currents', \n                     fontsize=14, fontweight='bold', pad=10)\n        ax1.set_xlabel('Time Point Index', fontsize=12, fontweight='bold')\n        ax1.set_ylabel('Input Current (A.U.)', fontsize=12, fontweight='bold')\n        ax1.legend(fontsize=11, loc='best')\n        ax1.grid(True, alpha=0.3, linestyle='--')\n        ax1.set_xlim([-0.5, n_timepoints-0.5])\n        \n        # Plot average over class\n        ax2 = fig.add_subplot(gs[label, 1])\n        class_samples = X_data[y_data == label]\n        mean_series = np.mean(class_samples, axis=0)\n        std_series = np.std(class_samples, axis=0)\n        \n        for neuron in range(d):\n            ax2.plot(range(n_timepoints), mean_series[neuron, :], linewidth=3, \n                    color=colors[neuron], label=f'Input Neuron {neuron+1}', alpha=0.9)\n            ax2.fill_between(range(n_timepoints),\n                           mean_series[neuron, :] - std_series[neuron, :],\n                           mean_series[neuron, :] + std_series[neuron, :],\n                           color=colors[neuron], alpha=0.2)\n        ax2.set_title(f'Class {label} Average (n={len(class_samples)} samples)\\n' + \n                     r'Mean $\\pm$ Standard Deviation', \n                     fontsize=14, fontweight='bold', pad=10)\n        ax2.set_xlabel('Time Point Index', fontsize=12, fontweight='bold')\n        ax2.set_ylabel('Input Current (A.U.)', fontsize=12, fontweight='bold')\n        ax2.legend(fontsize=11, loc='best')\n        ax2.grid(True, alpha=0.3, linestyle='--')\n        ax2.set_xlim([-0.5, n_timepoints-0.5])\n    \n    # Class distribution\n    ax3 = fig.add_subplot(gs[2, :])\n    class_counts = [np.sum(y_data == 0), np.sum(y_data == 1)]\n    bars = ax3.bar(['Class 0\\n(Negative)', 'Class 1\\n(Positive)'], class_counts, \n                   color=['#2196F3', '#F44336'], alpha=0.7, edgecolor='black', linewidth=2)\n    \n    # Add value labels on bars\n    for bar, count in zip(bars, class_counts):\n        height = bar.get_height()\n        ax3.text(bar.get_x() + bar.get_width()/2., height,\n                f'{count}\\n({count/N*100:.1f}%)',\n                ha='center', va='bottom', fontsize=14, fontweight='bold')\n    \n    ax3.set_ylabel('Number of Samples', fontsize=13, fontweight='bold')\n    ax3.set_title('Dataset Class Distribution\\n' + \n                 f'Total: {N} samples, {d} input neurons, {n_timepoints} time points', \n                 fontsize=14, fontweight='bold', pad=10)\n    ax3.grid(True, alpha=0.3, axis='y', linestyle='--')\n    ax3.set_ylim([0, max(class_counts) * 1.2])\n    \n    plt.savefig('data_timeseries.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: data_timeseries.png\")\n    plt.show()\n    return fig\n\n\ndef visualize_confusion_matrix(y_true, y_pred, filename='confusion_matrix.png'):\n    \"\"\"Explicit confusion matrix visualization\"\"\"\n    from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    \n    # Ensure binary labels\n    y_true = np.array(y_true).astype(int)\n    y_pred = np.array(y_pred).astype(int)\n    \n    # Compute confusion matrix\n    cm = sk_confusion_matrix(y_true, y_pred, labels=[0, 1])\n    \n    # Compute metrics with proper binary classification settings\n    accuracy = accuracy_score(y_true, y_pred)\n    \n    # For binary classification, specify pos_label=1 and zero_division=0\n    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n    \n    # Explicit breakdown\n    if cm.shape == (2, 2):\n        TN, FP, FN, TP = cm.ravel()\n    else:\n        # Handle edge cases\n        TN = cm[0, 0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0\n        FP = cm[0, 1] if cm.shape[0] > 0 and cm.shape[1] > 1 else 0\n        FN = cm[1, 0] if cm.shape[0] > 1 and cm.shape[1] > 0 else 0\n        TP = cm[1, 1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n    \n    # Confusion matrix heatmap\n    im = axes[0].imshow(cm, cmap='Blues', aspect='auto')\n    axes[0].set_title('Confusion Matrix\\n(Binary Classification)', \n                     fontsize=16, fontweight='bold', pad=20)\n    axes[0].set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('True Label', fontsize=14, fontweight='bold')\n    axes[0].set_xticks([0, 1])\n    axes[0].set_yticks([0, 1])\n    axes[0].set_xticklabels(['Negative (0)', 'Positive (1)'], fontsize=12)\n    axes[0].set_yticklabels(['Negative (0)', 'Positive (1)'], fontsize=12)\n    \n    # Add counts with labels\n    labels = [['TN', 'FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            value = cm[i, j] if i < cm.shape[0] and j < cm.shape[1] else 0\n            text_color = \"white\" if value > cm.max()/2 else \"black\"\n            \n            # Main count\n            axes[0].text(j, i, f'{value}',\n                        ha=\"center\", va=\"center\", \n                        color=text_color,\n                        fontsize=32, fontweight='bold')\n            \n            # Label (TN, FP, etc.)\n            axes[0].text(j, i-0.35, f'({labels[i][j]})',\n                        ha=\"center\", va=\"center\",\n                        color=text_color,\n                        fontsize=14, style='italic')\n    \n    plt.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n    \n    # Metrics table\n    axes[1].axis('tight')\n    axes[1].axis('off')\n    \n    table_data = [\n        ['Metric', 'Value', 'Formula'],\n        ['', '', ''],\n        ['True Negative (TN)', f'{TN}', 'Correctly predicted as 0'],\n        ['False Positive (FP)', f'{FP}', 'Incorrectly predicted as 1'],\n        ['False Negative (FN)', f'{FN}', 'Incorrectly predicted as 0'],\n        ['True Positive (TP)', f'{TP}', 'Correctly predicted as 1'],\n        ['', '', ''],\n        ['Accuracy', f'{accuracy:.4f}', '(TP + TN) / Total'],\n        ['Precision', f'{precision:.4f}', 'TP / (TP + FP)'],\n        ['Recall (Sensitivity)', f'{recall:.4f}', 'TP / (TP + FN)'],\n        ['F1-Score', f'{f1:.4f}', '2·P·R / (P + R)'],\n        ['', '', ''],\n        ['Total Samples', f'{len(y_true)}', ''],\n    ]\n    \n    table = axes[1].table(cellText=table_data, cellLoc='left',\n                         loc='center', bbox=[0.05, 0.1, 0.9, 0.85])\n    table.auto_set_font_size(False)\n    table.set_fontsize(11)\n    table.scale(1, 1.8)\n    \n    # Style header\n    for i in range(3):\n        table[(0, i)].set_facecolor('#2196F3')\n        table[(0, i)].set_text_props(weight='bold', color='white', size=13)\n    \n    # Style section headers\n    for i in [2, 7, 12]:\n        if i < len(table_data):\n            table[(i, 0)].set_text_props(weight='bold', size=12)\n    \n    # Highlight metrics rows\n    for i in [7, 8, 9, 10]:\n        if i < len(table_data):\n            table[(i, 0)].set_facecolor('#E3F2FD')\n            table[(i, 1)].set_facecolor('#E3F2FD')\n            table[(i, 2)].set_facecolor('#E3F2FD')\n    \n    plt.suptitle(f'Classification Performance (Accuracy: {accuracy:.1%})', \n                fontsize=18, fontweight='bold', y=0.96)\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.94])\n    plt.savefig(filename, dpi=150, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    print(f\"\\n\" + \"=\"*60)\n    print(\"CONFUSION MATRIX BREAKDOWN\")\n    print(\"=\"*60)\n    print(f\"  True Negative (TN):   {TN:4d}  (Correctly predicted class 0)\")\n    print(f\"  False Positive (FP):  {FP:4d}  (Wrongly predicted class 1)\")\n    print(f\"  False Negative (FN):  {FN:4d}  (Wrongly predicted class 0)\")\n    print(f\"  True Positive (TP):   {TP:4d}  (Correctly predicted class 1)\")\n    print(f\"\\n\" + \"=\"*60)\n    print(\"PERFORMANCE METRICS\")\n    print(\"=\"*60)\n    print(f\"  Accuracy:   {accuracy:.4f}  ({accuracy:.1%})\")\n    print(f\"  Precision:  {precision:.4f}  ({precision:.1%})\")\n    print(f\"  Recall:     {recall:.4f}  ({recall:.1%})\")\n    print(f\"  F1-Score:   {f1:.4f}  ({f1:.1%})\")\n    print(\"=\"*60)\n    plt.show()\n    return fig\n\n\ndef visualize_shooting_convergence(costs, filename='shooting_convergence.png'):\n    \"\"\"Visualize shooting method convergence with clear labels for presentation\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    iterations = range(len(costs))\n    \n    # Linear scale\n    axes[0].plot(iterations, costs, 'bo-', linewidth=2.5, markersize=10, \n                markerfacecolor='blue', markeredgecolor='darkblue', markeredgewidth=2)\n    axes[0].set_xlabel('Iteration Number', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel(r'Cost Function $J(\\lambda(0)) = \\frac{1}{2}\\|\\lambda(T) - \\lambda_T^*\\|^2$', \n                      fontsize=13, fontweight='bold')\n    axes[0].set_title('Shooting Method Convergence\\n(Linear Scale)', \n                     fontsize=15, fontweight='bold', pad=15)\n    axes[0].grid(True, alpha=0.4, linestyle='--', linewidth=1)\n    axes[0].set_xlim(-1, len(costs))\n    \n    # Add convergence annotation\n    if len(costs) > 1:\n        final_cost = costs[-1]\n        axes[0].axhline(y=final_cost, color='green', linestyle='--', \n                       linewidth=2, alpha=0.7, label=f'Final cost: {final_cost:.2e}')\n        axes[0].legend(fontsize=12, loc='upper right')\n    \n    # Log scale\n    axes[1].semilogy(iterations, costs, 'ro-', linewidth=2.5, markersize=10,\n                    markerfacecolor='red', markeredgecolor='darkred', markeredgewidth=2)\n    axes[1].set_xlabel('Iteration Number', fontsize=14, fontweight='bold')\n    axes[1].set_ylabel(r'Cost Function $J(\\lambda(0))$ (Log Scale)', \n                      fontsize=13, fontweight='bold')\n    axes[1].set_title('Shooting Method Convergence\\n(Logarithmic Scale)', \n                     fontsize=15, fontweight='bold', pad=15)\n    axes[1].grid(True, alpha=0.4, linestyle='--', linewidth=1, which='both')\n    axes[1].set_xlim(-1, len(costs))\n    \n    # Add convergence threshold line\n    threshold = 1e-6\n    axes[1].axhline(y=threshold, color='green', linestyle='--', \n                   linewidth=2, alpha=0.7, label=f'Tolerance: {threshold:.0e}')\n    axes[1].legend(fontsize=12, loc='upper right')\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    plt.show()\n    return fig\n\n\ndef visualize_linearized_system(X_traj, A_traj, t_sample, config, filename='linearized_system.png'):\n    \"\"\"Visualize linearized system with explicit layer labels\"\"\"\n    n = A_traj.shape[1]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    \n    # Plot 1: Input neuron (Layer 0)\n    axes[0, 0].plot(t_sample, X_traj[:, 0], 'b-', linewidth=2.5)\n    axes[0, 0].set_title('Input Layer: Neuron Membrane Potential\\n' + r'$v(t)$ (Layer 0)', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[0, 0].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    axes[0, 0].set_ylabel(r'Membrane Potential $v(t)$', fontsize=12, fontweight='bold')\n    axes[0, 0].axhline(y=config.theta_v, color='r', linestyle='--', \n                      linewidth=2, alpha=0.7, label=f'Threshold θ_v={config.theta_v}')\n    axes[0, 0].grid(True, alpha=0.3, linestyle='--')\n    axes[0, 0].legend(fontsize=11)\n    \n    # Plot 2: Hidden layer neurons (Layer 1)\n    colors_hidden = ['#2E7D32', '#C62828', '#1565C0']\n    for p in range(min(3, config.P)):\n        axes[0, 1].plot(t_sample, X_traj[:, 1+p], linewidth=2.5, \n                       color=colors_hidden[p], label=f'ξ₁,{p+1}')\n    axes[0, 1].set_title('Hidden Layer 1: Neuron Membrane Potentials\\n' + r'$\\xi_{1,p}(t)$ (Layer 1)', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[0, 1].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    axes[0, 1].set_ylabel(r'Membrane Potential $\\xi(t)$', fontsize=12, fontweight='bold')\n    axes[0, 1].axhline(y=config.theta_h, color='orange', linestyle='--', \n                      linewidth=2, alpha=0.7, label=f'Threshold θ_h={config.theta_h}')\n    axes[0, 1].legend(fontsize=11)\n    axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n    \n    # Plot 3: Output layer neurons\n    output_start_idx = 1 + config.L * config.P\n    colors_output = ['#6A1B9A', '#D84315']\n    for c in range(config.n_classes):\n        if output_start_idx + c < X_traj.shape[1]:\n            axes[0, 2].plot(t_sample, X_traj[:, output_start_idx + c], linewidth=2.5,\n                           color=colors_output[c], label=f'u_{c+1}')\n    axes[0, 2].set_title('Output Layer: Membrane Potentials\\n' + r'$u_c(t)$ (Final Layer)', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[0, 2].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    axes[0, 2].set_ylabel(r'Membrane Potential $u(t)$', fontsize=12, fontweight='bold')\n    axes[0, 2].axhline(y=config.theta_u, color='purple', linestyle='--', \n                      linewidth=2, alpha=0.7, label=f'Threshold θ_u={config.theta_u}')\n    axes[0, 2].legend(fontsize=11)\n    axes[0, 2].grid(True, alpha=0.3, linestyle='--')\n    \n    # Plot 4: Jacobian A(t) at t=T/2\n    mid_idx = len(A_traj) // 2\n    im = axes[1, 0].imshow(A_traj[mid_idx], cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n    axes[1, 0].set_title(r'Linearized Dynamics: Jacobian $\\mathbf{A}(t)$' + f'\\nat t={t_sample[mid_idx]:.1f}s', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[1, 0].set_xlabel('State Component j', fontsize=12, fontweight='bold')\n    axes[1, 0].set_ylabel('State Component i', fontsize=12, fontweight='bold')\n    \n    # Add layer annotations\n    layer_boundaries = [0, 1, 1 + config.L*config.P, n]\n    layer_names = ['Input', 'Hidden', 'Output']\n    for i, (start, end) in enumerate(zip(layer_boundaries[:-1], layer_boundaries[1:])):\n        mid = (start + end) / 2\n        if i < len(layer_names):\n            axes[1, 0].text(-1.5, mid, layer_names[i], fontsize=10, \n                          rotation=90, va='center', ha='right', fontweight='bold')\n            axes[1, 0].text(mid, -1.5, layer_names[i], fontsize=10, \n                          rotation=0, ha='center', va='top', fontweight='bold')\n    \n    cbar = plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)\n    cbar.set_label(r'$\\partial F_i / \\partial X_j$', fontsize=11, fontweight='bold')\n    \n    # Plot 5: Eigenvalue evolution of A(t)\n    eigenvalues = []\n    for A in A_traj:\n        eigvals = np.linalg.eigvals(A)\n        # Sort by real part for consistent tracking\n        eigvals = eigvals[np.argsort(-np.real(eigvals))]\n        eigenvalues.append(eigvals)\n    eigenvalues = np.array(eigenvalues)\n    \n    colors_eig = plt.cm.viridis(np.linspace(0, 1, min(8, n)))\n    for i in range(min(8, n)):\n        axes[1, 1].plot(t_sample, np.real(eigenvalues[:, i]), linewidth=2.5, \n                       color=colors_eig[i], label=f'λ_{i+1}', alpha=0.8)\n    axes[1, 1].set_title(r'Eigenvalues of Jacobian $\\mathbf{A}(t)$' + '\\n(Real Part)', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[1, 1].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    axes[1, 1].set_ylabel(r'$\\mathrm{Re}(\\lambda_i)$', fontsize=12, fontweight='bold')\n    axes[1, 1].legend(fontsize=10, ncol=2, loc='best')\n    axes[1, 1].grid(True, alpha=0.3, linestyle='--')\n    axes[1, 1].axhline(y=0, color='k', linestyle='-', linewidth=1.5, alpha=0.5)\n    \n    # Plot 6: Frobenius norm of A(t)\n    A_norms = [np.linalg.norm(A, 'fro') for A in A_traj]\n    axes[1, 2].plot(t_sample, A_norms, 'b-', linewidth=2.5)\n    axes[1, 2].fill_between(t_sample, 0, A_norms, alpha=0.3)\n    axes[1, 2].set_title(r'Jacobian Magnitude: $\\|\\mathbf{A}(t)\\|_F$' + '\\n(Frobenius Norm)', \n                        fontsize=14, fontweight='bold', pad=10)\n    axes[1, 2].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    axes[1, 2].set_ylabel(r'$\\|\\mathbf{A}(t)\\|_F$', fontsize=12, fontweight='bold')\n    axes[1, 2].grid(True, alpha=0.3, linestyle='--')\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    plt.show()\n    return fig\n\n\ndef visualize_adjoint_trajectories(X_traj, lambda_traj, t_grid, config, filename='adjoint_trajectories.png'):\n    \"\"\"Visualize adjoint trajectories with explicit layer labels\"\"\"\n    n_state = X_traj.shape[1]\n    \n    # Define layer structure\n    layers = []\n    layers.append(('Input Layer', 0, 1))  # v\n    layers.append(('Hidden Layer 1', 1, 1 + config.P))  # ξ_1\n    if config.L > 1:\n        layers.append(('Hidden Layer 2', 1 + config.P, 1 + 2*config.P))  # ξ_2\n    layers.append(('Output Layer', 1 + config.L*config.P, n_state))  # u\n    \n    # Create subplots for each layer\n    n_layers = len(layers)\n    fig, axes = plt.subplots(n_layers, 2, figsize=(18, 4*n_layers))\n    \n    if n_layers == 1:\n        axes = axes.reshape(1, -1)\n    \n    for layer_idx, (layer_name, start_idx, end_idx) in enumerate(layers):\n        n_neurons = end_idx - start_idx\n        colors = plt.cm.tab10(np.linspace(0, 1, n_neurons))\n        \n        # Forward state\n        for i, idx in enumerate(range(start_idx, end_idx)):\n            label = ''\n            if layer_name == 'Input Layer':\n                label = 'v(t)'\n            elif 'Hidden' in layer_name:\n                layer_num = int(layer_name.split()[-1])\n                neuron_num = i + 1\n                label = f'ξ_{layer_num},{neuron_num}(t)'\n            elif layer_name == 'Output Layer':\n                label = f'u_{i+1}(t)'\n            \n            axes[layer_idx, 0].plot(t_grid, X_traj[:, idx], linewidth=2, \n                                   color=colors[i], label=label, alpha=0.8)\n        \n        axes[layer_idx, 0].set_ylabel('State Value', fontsize=11, fontweight='bold')\n        axes[layer_idx, 0].set_title(f'{layer_name}: Forward State\\n' + r'$X(t)$', \n                                     fontsize=13, fontweight='bold', pad=10)\n        axes[layer_idx, 0].legend(fontsize=10, loc='best', ncol=min(3, n_neurons))\n        axes[layer_idx, 0].grid(True, alpha=0.3, linestyle='--')\n        \n        # Adjoint state\n        for i, idx in enumerate(range(start_idx, end_idx)):\n            label = ''\n            if layer_name == 'Input Layer':\n                label = 'λ_v(t)'\n            elif 'Hidden' in layer_name:\n                layer_num = int(layer_name.split()[-1])\n                neuron_num = i + 1\n                label = f'λ_{layer_num},{neuron_num}(t)'\n            elif layer_name == 'Output Layer':\n                label = f'λ_u{i+1}(t)'\n            \n            axes[layer_idx, 1].plot(t_grid, lambda_traj[:, idx], linewidth=2, \n                                   color=colors[i], label=label, alpha=0.8)\n        \n        axes[layer_idx, 1].set_ylabel('Adjoint Value', fontsize=11, fontweight='bold')\n        axes[layer_idx, 1].set_title(f'{layer_name}: Adjoint State\\n' + r'$\\lambda(t)$', \n                                     fontsize=13, fontweight='bold', pad=10)\n        axes[layer_idx, 1].legend(fontsize=10, loc='best', ncol=min(3, n_neurons))\n        axes[layer_idx, 1].grid(True, alpha=0.3, linestyle='--')\n        \n        # X-labels only on bottom row\n        if layer_idx == n_layers - 1:\n            axes[layer_idx, 0].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n            axes[layer_idx, 1].set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    plt.show()\n    return fig\n\n\ndef visualize_training_history(history, filename='training_history.png'):\n    \"\"\"Visualize training history\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    epochs = [h['epoch'] for h in history]\n    losses = [h['loss'] for h in history]\n    accuracies = [h['accuracy'] for h in history]\n    zetas = [h['zeta'] for h in history]\n    \n    axes[0, 0].plot(epochs, losses, 'b-o', linewidth=2, markersize=6)\n    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n    axes[0, 0].set_ylabel('Loss', fontsize=12)\n    axes[0, 0].set_title('Training Loss', fontsize=13, fontweight='bold')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    axes[0, 1].plot(epochs, accuracies, 'g-o', linewidth=2, markersize=6)\n    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n    axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n    axes[0, 1].set_title('Training Accuracy', fontsize=13, fontweight='bold')\n    axes[0, 1].set_ylim([0, 1.1])\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    axes[1, 0].plot(epochs, zetas, 'r-o', linewidth=2, markersize=6)\n    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n    axes[1, 0].set_ylabel('ζ', fontsize=12)\n    axes[1, 0].set_title('Mollification Schedule', fontsize=13, fontweight='bold')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    axes[1, 1].semilogy(epochs, losses, 'b-o', linewidth=2, markersize=6)\n    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n    axes[1, 1].set_ylabel('Loss (log)', fontsize=12)\n    axes[1, 1].set_title('Training Loss (Log Scale)', fontsize=13, fontweight='bold')\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    plt.show()\n    return fig\n\n\ndef visualize_output_neurons_with_spikes(snn, X_timeseries, config, filename='output_spikes.png'):\n    \"\"\"\n    Visualize output neuron membrane potentials with threshold lines.\n    Shows where actual output pulses would occur.\n    \"\"\"\n    print(\"  Simulating to visualize output spikes...\")\n    \n    # Simulate to get full trajectory\n    t_grid, X_traj = snn.simulate(X_timeseries, T=config.T, record=True)\n    \n    # Extract output neuron indices\n    output_start_idx = 1 + config.L * config.P\n    \n    fig, axes = plt.subplots(config.n_classes, 1, figsize=(16, 4*config.n_classes), squeeze=False)\n    \n    colors = ['#6A1B9A', '#D84315']\n    \n    for c in range(config.n_classes):\n        ax = axes[c, 0]\n        \n        # Plot membrane potential\n        u_trace = X_traj[:, output_start_idx + c]\n        ax.plot(t_grid, u_trace, linewidth=2.5, color=colors[c], \n               label=f'Output Neuron {c+1}: u_{c+1}(t)')\n        \n        # Plot threshold\n        ax.axhline(y=config.theta_u, color='red', linestyle='--', \n                  linewidth=2.5, alpha=0.8, label=f'Threshold θ_u = {config.theta_u}')\n        \n        # Detect and mark threshold crossings (spikes)\n        crossings = []\n        for i in range(len(u_trace) - 1):\n            if u_trace[i] < config.theta_u and u_trace[i+1] >= config.theta_u:\n                crossings.append(i+1)\n        \n        # Mark spike times with vertical lines\n        for spike_idx in crossings:\n            ax.axvline(x=t_grid[spike_idx], color='orange', linestyle=':', \n                      linewidth=2, alpha=0.6)\n        \n        # Add spike markers\n        if len(crossings) > 0:\n            spike_times = t_grid[crossings]\n            spike_values = u_trace[crossings]\n            ax.plot(spike_times, spike_values, 'o', color='orange', \n                   markersize=12, markeredgewidth=2, markeredgecolor='red',\n                   label=f'Spike Events ({len(crossings)} spikes)', zorder=5)\n        \n        # Styling\n        ax.set_ylabel(r'Membrane Potential $u_{' + str(c+1) + '}(t)$', \n                     fontsize=13, fontweight='bold')\n        ax.set_title(f'Output Neuron {c+1}: Membrane Potential & Spike Detection\\n' + \n                    f'(Class {c} output)', fontsize=14, fontweight='bold', pad=10)\n        ax.legend(fontsize=12, loc='upper right', framealpha=0.9)\n        ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)\n        ax.set_xlim([0, config.T])\n        \n        # Add shaded region above threshold\n        ax.fill_between(t_grid, config.theta_u, ax.get_ylim()[1], \n                       color='red', alpha=0.05, label='Spiking Region')\n        \n        # Only add x-label to bottom plot\n        if c == config.n_classes - 1:\n            ax.set_xlabel('Time (s)', fontsize=13, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    print(f\"  Output neuron spike counts:\")\n    for c in range(config.n_classes):\n        u_trace = X_traj[:, output_start_idx + c]\n        n_spikes = sum(1 for i in range(len(u_trace)-1) \n                      if u_trace[i] < config.theta_u and u_trace[i+1] >= config.theta_u)\n        print(f\"    Neuron {c+1}: {n_spikes} spikes\")\n    plt.show()\n    return fig\n\n\ndef run_complete_analysis():\n    \"\"\"Complete analysis with corrected shooting method and time-series input\"\"\"\n    \n    print(\"=\"*80)\n    print(\"PHASE 1: LINEARIZED PMP VIA SHOOTING METHOD\")\n    print(\"=\"*80)\n    \n    # Load time-series data\n    X_data, y_data, n_classes = load_data_from_csv('lif.csv')\n    N = len(X_data)\n    \n    # Visualize time-series data\n    visualize_data(X_data, y_data)\n    \n    # Create configuration\n    config = NetworkConfig()\n    config.n_classes = n_classes\n    config.d = X_data.shape[1]  # Number of input neurons\n    config.n_timepoints = X_data.shape[2]  # Number of time points\n    \n    print(f\"\\nNetwork Configuration:\")\n    print(f\"  Input neurons: {config.d}\")\n    print(f\"  Time points: {config.n_timepoints}\")\n    print(f\"  Hidden layers: {config.L}\")\n    print(f\"  Neurons per layer: {config.P}\")\n    print(f\"  Output classes: {config.n_classes}\")\n    print(f\"  State dimension: {config.n_state}\")\n    print(f\"  Time horizon: {config.T}s (visualization), {config.T_train}s (training)\")\n    \n    # Create network\n    snn = SNNDynamics(config)\n    snn.update_zeta(0, 5)\n    \n    # Step 1: Linearize system\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 1: Linearizing SNN dynamics\")\n    print(\"=\"*80)\n    \n    linear_sys = LinearizedSystem(snn)\n    X_ref, A_traj = linear_sys.linearize_along_trajectory(X_data[0])\n    \n    # Visualize linearized system\n    visualize_linearized_system(X_ref, A_traj, linear_sys.t_grid, config)\n    \n    # Step 2: Solve adjoint with proper shooting method\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: Solving linearized adjoint via shooting method\")\n    print(\"       (with linear interpolation for guesses)\")\n    print(\"=\"*80)\n    \n    lambda_T = np.zeros(config.n_state)\n    lambda_T[-n_classes + y_data[0]] = 1.0\n    \n    lambda_traj, shooting_costs = linear_sys.solve_adjoint_shooting(lambda_T, method='secant')\n    \n    # Visualize shooting convergence\n    visualize_shooting_convergence(shooting_costs, 'shooting_convergence.png')\n    \n    # Visualize adjoint solution\n    visualize_adjoint_trajectories(X_ref, lambda_traj, linear_sys.t_grid, config,\n                                   'linearized_adjoint.png')\n    \n    # Visualize output neurons with threshold and spikes\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: Visualizing output neurons with spike detection\")\n    print(\"=\"*80)\n    visualize_output_neurons_with_spikes(snn, X_data[0], config, 'output_spikes.png')\n    \n    # Step 3: Full nonlinear training\n    print(\"\\n\" + \"=\"*80)\n    print(\"PHASE 2: FULL NONLINEAR TRAINING\")\n    print(\"=\"*80)\n    \n    # Use subset\n    max_train_samples = min(100, N)\n    if N > max_train_samples:\n        indices = np.random.choice(N, max_train_samples, replace=False)\n        X_train = X_data[indices]\n        y_train = y_data[indices]\n    else:\n        X_train = X_data\n        y_train = y_data\n    \n    print(f\"Training set: {len(X_train)} samples\")\n    print(f\"Class distribution: class 0={np.sum(y_train==0)}, class 1={np.sum(y_train==1)}\")\n    \n    solver = ShootingMethodSolver(snn)\n    \n    n_epochs = 10\n    learning_rate = 0.005\n    gamma = 0.0001\n    \n    print(f\"\\nTraining for {n_epochs} epochs...\")\n    print(f\"  Learning rate: {learning_rate}\")\n    print(f\"  Regularization: {gamma}\")\n    \n    history = []\n    best_accuracy = 0.0\n    patience = 3\n    patience_counter = 0\n    \n    for epoch in range(n_epochs):\n        print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n        start_time = time.time()\n        \n        zeta = snn.update_zeta(epoch, n_epochs)\n        \n        try:\n            loss, accuracy = solver.train_step(X_train, y_train, learning_rate, gamma)\n            elapsed = time.time() - start_time\n            \n            history.append({\n                'epoch': epoch,\n                'loss': loss,\n                'accuracy': accuracy,\n                'zeta': zeta\n            })\n            \n            print(f\"  Loss={loss:.6f}, Acc={accuracy:.3f}, ζ={zeta:.2f}, Time={elapsed:.1f}s\")\n            \n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                patience_counter = 0\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= patience:\n                print(f\"  Early stopping: no improvement for {patience} epochs\")\n                break\n        except Exception as e:\n            print(f\"  Error in training: {e}\")\n            break\n    \n    # Visualize training history\n    if len(history) > 0:\n        visualize_training_history(history)\n    \n    # Compute predictions for confusion matrix\n    print(\"\\n\" + \"=\"*80)\n    print(\"Evaluating and computing confusion matrix...\")\n    print(\"=\"*80)\n    \n    y_pred = []\n    for i in tqdm(range(len(X_train)), desc=\"Predictions\"):\n        X_final = snn.simulate_fast(X_train[i], snn.config.T_train)\n        probs = snn.compute_output(X_final)\n        pred = np.argmax(probs)\n        y_pred.append(pred)\n    \n    y_pred = np.array(y_pred)\n    \n    # Visualize explicit confusion matrix\n    visualize_confusion_matrix(y_train, y_pred, 'confusion_matrix.png')\n    \n    # Final adjoint solution\n    print(\"\\n\" + \"=\"*80)\n    print(\"Computing final nonlinear adjoint solution...\")\n    print(\"=\"*80)\n    \n    X_final, lambda_final = solver.solve_forward_backward(X_train[0], y_train[0])\n    visualize_adjoint_trajectories(X_final, lambda_final, snn.t_grid, config,\n                                   'final_adjoint.png')\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ANALYSIS COMPLETE!\")\n    print(\"=\"*80)\n    print(f\"\\nGenerated figures (suitable for A0 presentation):\")\n    print(\"  1. data_timeseries.png       - Time-series input data\")\n    print(\"  2. linearized_system.png     - Linearization with all layers\")\n    print(\"  3. shooting_convergence.png  - Shooting method iterations\")\n    print(\"  4. linearized_adjoint.png    - Adjoint trajectories (linearized)\")\n    print(\"  5. output_spikes.png         - Output neurons with threshold\")\n    print(\"  6. training_history.png      - Training metrics\")\n    print(\"  7. confusion_matrix.png      - Explicit confusion matrix\")\n    print(\"  8. final_adjoint.png         - Final adjoint solution\")\n\n\n#if __name__ == \"__main__\":\n#    run_complete_analysis()","block_group":"e598a8d79df24dfb84faa938db97840f","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null}],
        "metadata": {"deepnote_persisted_session":{"createdAt":"2025-12-04T20:20:13.694Z"},"deepnote_notebook_id":"85a6ca97c9ab4511abd7e6c4610b7c07"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }