{"cells":[{"cellId":"6cbba2e6997047b391564eb67767ac54","cell_type":"code","metadata":{"cell_id":"6cbba2e6997047b391564eb67767ac54","deepnote_cell_type":"code"},"source":"\"\"\"\nShooting Method for SNN Training via Pontryagin's Maximum Principle\n====================================================================\n\nPriority 1: Solve linearized PMP using shooting method\nPriority 2: Full nonlinear training with multi-class support\n\nMulti-class classification with cross-entropy loss.\n\"\"\"\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import UnivariateSpline, interp1d\nfrom scipy.optimize import minimize, fsolve\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Callable, Optional\nimport pandas as pd\nimport os\nimport time\nimport warnings\nfrom concurrent.futures import ProcessPoolExecutor, TimeoutError as FuturesTimeoutError\nfrom functools import partial\nimport signal\n\n# For confusion matrix\ntry:\n    from sklearn.metrics import confusion_matrix\nexcept ImportError:\n    def confusion_matrix(y_true, y_pred):\n        \"\"\"Simple confusion matrix implementation\"\"\"\n        classes = np.unique(np.concatenate([y_true, y_pred]))\n        n_classes = len(classes)\n        cm = np.zeros((n_classes, n_classes), dtype=int)\n        for i, true_class in enumerate(classes):\n            for j, pred_class in enumerate(classes):\n                cm[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))\n        return cm\n\n# Progress bar\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable, **kwargs):\n        return iterable\n\nwarnings.filterwarnings('ignore')\n\n\nclass TimeoutException(Exception):\n    \"\"\"Exception raised when computation times out\"\"\"\n    pass\n\n\ndef timeout_handler(signum, frame):\n    \"\"\"Signal handler for timeout\"\"\"\n    raise TimeoutException(\"Computation timed out\")\n\n\n@dataclass\nclass NetworkConfig:\n    \"\"\"Network architecture configuration\"\"\"\n    d: int = 2              # Input dimension\n    L: int = 2              # Number of hidden layers\n    P: int = 3              # Neurons per hidden layer\n    n_classes: int = 1      # Number of output classes (set from data)\n    \n    # Time parameters\n    T: float = 60.0         # Time horizon (for testing/visualization)\n    T_train: float = 300    # Time horizon for training (much shorter!)\n    dt: float = 0.01        # Time step\n    \n    # Timeout settings\n    max_simulation_time: float = 10.0  # Max seconds per forward simulation\n    max_gradient_time: float = 30.0    # Max seconds per gradient computation\n    \n    # Training settings\n    batch_size: int = 16    # Mini-batch size\n    use_parallel: bool = True  # Parallel processing\n    n_workers: int = 4      # Number of parallel workers\n    \n    # Input layer parameters\n    tau_v: float = 8.0\n    theta_v: float = 0.8\n    \n    # Hidden layer parameters\n    tau_h: float = 6.0\n    theta_h: float = 0.25\n    \n    # Output layer parameters\n    tau_u: float = 10.0\n    theta_u: float = 0.3\n    \n    # Spike kernel\n    mu: float = 0.2\n    \n    # Mollification schedule\n    zeta_0: float = 3.0\n    zeta_1: float = 10.0\n    \n    def get_zeta(self, epoch: int, total_epochs: int) -> float:\n        \"\"\"Geometric schedule for ζ\"\"\"\n        if total_epochs <= 1:\n            return self.zeta_1\n        return self.zeta_0 * (self.zeta_1 / self.zeta_0) ** (epoch / (total_epochs - 1))\n    \n    @property\n    def n_state(self) -> int:\n        \"\"\"Total state dimension: 1 input + L*P hidden + n_classes output\"\"\"\n        return 1 + self.L * self.P + self.n_classes\n\n\nclass GaussianKernel:\n    \"\"\"Gaussian spike kernel\"\"\"\n    \n    def __init__(self, mu: float = 0.2):\n        self.mu = mu\n        self.coef = 1.0 / (mu * np.sqrt(2 * np.pi))\n    \n    def __call__(self, t: float, spike_times: List[float]) -> float:\n        if len(spike_times) == 0:\n            return 0.0\n        return sum(self.coef * np.exp(-0.5 * ((t - ts) / self.mu) ** 2) \n                   for ts in spike_times)\n\n\nclass MollifiedReset:\n    \"\"\"Mollified reset function\"\"\"\n    \n    def __init__(self, zeta: float = 10.0):\n        self.zeta = zeta\n    \n    def H(self, s: np.ndarray) -> np.ndarray:\n        \"\"\"H_ζ(s) = 1/2(1 + tanh(ζs/2))\"\"\"\n        return 0.5 * (1 + np.tanh(self.zeta * s / 2))\n    \n    def D(self, s: np.ndarray, theta: float) -> np.ndarray:\n        \"\"\"D_ζ(s; θ) = (1 - H_ζ(s - θ))s\"\"\"\n        return (1 - self.H(s - theta)) * s\n    \n    def dD_ds(self, s: np.ndarray, theta: float) -> np.ndarray:\n        \"\"\"Derivative ∂D/∂s\"\"\"\n        H_val = self.H(s - theta)\n        dH_ds = 0.25 * self.zeta * (1 / np.cosh(self.zeta * (s - theta) / 2))**2\n        return (1 - H_val) - s * (dH_ds)\n\n\nclass SNNDynamics:\n    \"\"\"SNN dynamics with multi-class output\"\"\"\n    \n    def __init__(self, config: NetworkConfig):\n        self.config = config\n        self.kernel = GaussianKernel(config.mu)\n        self.reset = MollifiedReset()\n        \n        # Initialize parameters\n        self.a = np.random.randn(config.d) * 0.1\n        \n        self.omega = []\n        for ell in range(config.L):\n            if ell == 0:\n                self.omega.append(np.random.randn(config.P, 1) * 0.1)\n            else:\n                self.omega.append(np.random.randn(config.P, config.P) * 0.1)\n        \n        self.w = np.random.randn(config.n_classes, config.P) * 0.1\n        self.nu = np.random.randn(config.n_classes) * 0.1\n        \n        # Spike times storage\n        self.spike_times_input = []\n        self.spike_times_hidden = [[[] for _ in range(config.P)] \n                                   for _ in range(config.L)]\n        \n        # Trajectory cache\n        self.X_trajectory = None\n        self.t_grid = None\n    \n    def reset_spikes(self):\n        \"\"\"Clear spike times\"\"\"\n        self.spike_times_input = []\n        self.spike_times_hidden = [[[] for _ in range(self.config.P)] \n                                   for _ in range(self.config.L)]\n    \n    def update_zeta(self, epoch: int, total_epochs: int):\n        \"\"\"Update mollification parameter\"\"\"\n        zeta = self.config.get_zeta(epoch, total_epochs)\n        self.reset.zeta = zeta\n        return zeta\n    \n    def dynamics(self, t: float, X: np.ndarray, x_input: np.ndarray) -> np.ndarray:\n        \"\"\"Compute dX/dt = F(X, Υ)\"\"\"\n        cfg = self.config\n        dX = np.zeros_like(X)\n        idx = 0\n        \n        # Input neuron\n        v = X[idx]\n        dX[idx] = (1.0 / cfg.tau_v) * (-v + np.dot(self.a, x_input))\n        idx += 1\n        \n        # Hidden layers\n        for ell in range(cfg.L):\n            for p in range(cfg.P):\n                xi = X[idx]\n                \n                if ell == 0:\n                    J = self.kernel(t, self.spike_times_input)\n                    synaptic_input = self.omega[ell][p, 0] * J\n                else:\n                    synaptic_input = 0.0\n                    for q in range(cfg.P):\n                        J_q = self.kernel(t, self.spike_times_hidden[ell-1][q])\n                        synaptic_input += self.omega[ell][p, q] * J_q\n                \n                dX[idx] = (1.0 / cfg.tau_h) * (-xi + synaptic_input)\n                idx += 1\n        \n        # Output layer (multi-class)\n        for c in range(cfg.n_classes):\n            u = X[idx]\n            \n            Phi = 0.0\n            for q in range(cfg.P):\n                Phi += self.kernel(t, self.spike_times_hidden[-1][q])\n            \n            dX[idx] = (1.0 / cfg.tau_u) * (-u + np.dot(self.w[c, :], np.ones(cfg.P)) * Phi)\n            idx += 1\n        \n        return dX\n    \n    def simulate(self, x_input: np.ndarray, \n                 T: Optional[float] = None,\n                 record: bool = True,\n                 timeout: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Forward simulation with optional timeout.\n        \n        Parameters:\n        -----------\n        timeout : float, optional\n            Maximum time in seconds for simulation\n        \"\"\"\n        self.reset_spikes()\n        \n        if T is None:\n            T = self.config.T\n        \n        start_time = time.time()\n        \n        dt = self.config.dt\n        t_grid = np.arange(0, T + dt, dt)\n        n_steps = len(t_grid)\n        \n        X = np.zeros((n_steps, self.config.n_state))\n        X[0] = 0.0\n        \n        for i in range(n_steps - 1):\n            # Check timeout\n            if timeout is not None and (time.time() - start_time) > timeout:\n                print(f\"  Warning: Simulation timeout after {time.time() - start_time:.2f}s\")\n                # Return partial trajectory\n                if record:\n                    self.X_trajectory = X[:i+1]\n                    self.t_grid = t_grid[:i+1]\n                return t_grid[:i+1], X[:i+1]\n            \n            t = t_grid[i]\n            \n            dX = self.dynamics(t, X[i], x_input)\n            X[i+1] = X[i] + dX * dt\n            \n            idx = 0\n            \n            # Input neuron\n            if X[i+1, idx] >= self.config.theta_v:\n                self.spike_times_input.append(t)\n                X[i+1, idx] = self.reset.D(np.array([X[i+1, idx]]), \n                                          self.config.theta_v)[0]\n            idx += 1\n            \n            # Hidden neurons\n            for ell in range(self.config.L):\n                for p in range(self.config.P):\n                    if X[i+1, idx] >= self.config.theta_h:\n                        self.spike_times_hidden[ell][p].append(t)\n                        X[i+1, idx] = self.reset.D(np.array([X[i+1, idx]]), \n                                                   self.config.theta_h)[0]\n                    idx += 1\n            \n            idx += self.config.n_classes\n        \n        if record:\n            self.X_trajectory = X\n            self.t_grid = t_grid\n        \n        return t_grid, X\n    \n    def simulate_fast(self, x_input: np.ndarray, T: float) -> np.ndarray:\n        \"\"\"\n        Fast simulation for training (only returns final state).\n        \n        Returns:\n        --------\n        X_final : np.ndarray\n            Final state X(T)\n        \"\"\"\n        try:\n            _, X_traj = self.simulate(x_input, T=T, record=False, \n                                     timeout=self.config.max_simulation_time)\n            return X_traj[-1]\n        except Exception as e:\n            print(f\"  Error in simulation: {e}\")\n            return np.zeros(self.config.n_state)\n    \n    def compute_output(self, X_final: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute network output (softmax for multi-class).\n        \n        Returns:\n        --------\n        probs : np.ndarray (n_classes,)\n            Class probabilities\n        \"\"\"\n        u_final = X_final[-self.config.n_classes:]\n        \n        # Logits: Σ ν_c σ(u_c - θ_u)\n        sigma_u = 1.0 / (1.0 + np.exp(-(u_final - self.config.theta_u)))\n        logits = self.nu * sigma_u\n        \n        # Softmax\n        exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n        probs = exp_logits / np.sum(exp_logits)\n        \n        return probs\n    \n    def compute_loss(self, probs: np.ndarray, y_true: int) -> float:\n        \"\"\"\n        Cross-entropy loss for multi-class.\n        \n        L = -log(p_y)\n        \"\"\"\n        return -np.log(probs[y_true] + 1e-10)\n    \n    def compute_state_jacobian(self, t: float, X: np.ndarray, \n                               x_input: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n        \"\"\"Compute A(t) = ∂F/∂X\"\"\"\n        n = len(X)\n        A = np.zeros((n, n))\n        \n        F0 = self.dynamics(t, X, x_input)\n        \n        for i in range(n):\n            X_pert = X.copy()\n            X_pert[i] += eps\n            F_pert = self.dynamics(t, X_pert, x_input)\n            A[:, i] = (F_pert - F0) / eps\n        \n        return A\n\n\nclass LinearizedSystem:\n    \"\"\"\n    Linearized system around reference trajectory.\n    \n    δẋ = A(t)δx + B(t)δu\n    -δλ̇ = A(t)^T δλ\n    \"\"\"\n    \n    def __init__(self, snn: SNNDynamics):\n        self.snn = snn\n        self.A_traj = None  # Store A(t) trajectory\n        self.t_grid = None\n        self.X_traj = None  # Store full reference trajectory\n    \n    def linearize_along_trajectory(self, x_input: np.ndarray):\n        \"\"\"\n        Compute linearization A(t) along reference trajectory.\n        \"\"\"\n        print(\"  Linearizing system along reference trajectory...\")\n        \n        # Simulate reference trajectory\n        t_grid_full, X_traj_full = self.snn.simulate(x_input, record=True)\n        \n        # Subsample for efficiency\n        sample_rate = 10\n        t_sample = t_grid_full[::sample_rate]\n        X_sample = X_traj_full[::sample_rate]\n        \n        A_traj = []\n        for i, t in enumerate(t_sample):\n            A = self.snn.compute_state_jacobian(t, X_sample[i], x_input)\n            A_traj.append(A)\n        \n        self.A_traj = np.array(A_traj)\n        self.t_grid = t_sample\n        self.X_traj = X_sample  # Store subsampled trajectory\n        \n        print(f\"  Linearized at {len(t_sample)} time points\")\n        print(f\"  Reference trajectory shape: {X_sample.shape}\")\n        print(f\"  Time grid shape: {t_sample.shape}\")\n        \n        return X_sample, self.A_traj\n    \n    def create_A_interpolator(self) -> Callable:\n        \"\"\"Create interpolator for A(t)\"\"\"\n        n = self.A_traj.shape[1]\n        \n        # Flatten A matrices and interpolate each component\n        interpolators = []\n        for i in range(n):\n            for j in range(n):\n                A_ij = self.A_traj[:, i, j]\n                interp = interp1d(self.t_grid, A_ij, kind='cubic', \n                                 fill_value='extrapolate')\n                interpolators.append(interp)\n        \n        def A_interp(t):\n            A = np.zeros((n, n))\n            idx = 0\n            for i in range(n):\n                for j in range(n):\n                    A[i, j] = interpolators[idx](t)\n                    idx += 1\n            return A\n        \n        return A_interp\n    \n    def solve_adjoint_shooting(self, lambda_T: np.ndarray, \n                               lambda_0_guess: np.ndarray,\n                               method: str = 'newton') -> Tuple[np.ndarray, List[float]]:\n        \"\"\"\n        Solve adjoint equation using shooting method.\n        \n        Find λ(0) such that integrating -λ̇ = A(t)^T λ from t=0 to t=T\n        gives λ(T) = λ_T^target.\n        \n        Cost function: J(λ(0)) = ||λ(T; λ(0)) - λ_T^target||²\n        \n        Returns:\n        --------\n        lambda_traj : np.ndarray\n            Adjoint trajectory\n        costs : List[float]\n            Cost at each iteration\n        \"\"\"\n        print(f\"  Solving adjoint via shooting method ({method})...\")\n        print(f\"  Target λ(T) norm: {np.linalg.norm(lambda_T):.6f}\")\n        \n        A_interp = self.create_A_interpolator()\n        t_grid = self.t_grid\n        \n        def adjoint_rhs(t, lam):\n            \"\"\"−λ̇ = A(t)^T λ\"\"\"\n            A = A_interp(t)\n            return -A.T @ lam\n        \n        def forward_integrate_adjoint(lam_0):\n            \"\"\"Integrate adjoint forward from λ(0) to λ(T)\"\"\"\n            sol = solve_ivp(adjoint_rhs, [t_grid[0], t_grid[-1]], lam_0,\n                           t_eval=t_grid, method='RK45',\n                           rtol=1e-6, atol=1e-8)\n            return sol.y.T  # Shape: (n_time, n_state)\n        \n        def cost_function(lam_0):\n            \"\"\"Cost: ||λ(T) - λ_T^target||²\"\"\"\n            lam_traj = forward_integrate_adjoint(lam_0)\n            lam_T_computed = lam_traj[-1]\n            cost = 0.5 * np.sum((lam_T_computed - lambda_T)**2)\n            return cost, lam_traj\n        \n        # Initialize λ(0) by interpolating between zero and backward solution\n        print(f\"  Finding initial guess via backward integration...\")\n        \n        # Backward integration from λ(T) to get rough λ(0)\n        sol_backward = solve_ivp(adjoint_rhs, [t_grid[-1], t_grid[0]], lambda_T,\n                                 t_eval=t_grid[::-1], method='RK45',\n                                 rtol=1e-6, atol=1e-8)\n        lambda_0_backward = sol_backward.y[:, -1]  # λ(0) from backward\n        \n        print(f\"  Backward λ(0) norm: {np.linalg.norm(lambda_0_backward):.6f}\")\n        \n        # Use backward solution as initial guess\n        lambda_0_current = lambda_0_backward.copy()\n        \n        # Shooting method iterations\n        max_iter = 20\n        tol = 1e-6\n        costs = []\n        \n        print(f\"\\n  Starting shooting iterations...\")\n        print(f\"  {'Iter':<6} {'Cost':<12} {'||dλ||':<12} {'Step'}\")\n        print(f\"  {'-'*50}\")\n        \n        for iteration in range(max_iter):\n            cost, lam_traj = cost_function(lambda_0_current)\n            costs.append(cost)\n            \n            lam_T_computed = lam_traj[-1]\n            residual = lam_T_computed - lambda_T\n            residual_norm = np.linalg.norm(residual)\n            \n            print(f\"  {iteration:<6} {cost:<12.6e} {residual_norm:<12.6e}\", end='')\n            \n            if residual_norm < tol:\n                print(f\" ✓ Converged!\")\n                break\n            \n            # Newton step: compute sensitivity dλ(T)/dλ(0)\n            # Use finite differences\n            eps = 1e-6\n            n_state = len(lambda_0_current)\n            \n            # Compute Jacobian of terminal condition w.r.t. initial condition\n            dlam_T_dlam_0 = np.zeros((n_state, n_state))\n            \n            for i in range(n_state):\n                lam_0_pert = lambda_0_current.copy()\n                lam_0_pert[i] += eps\n                lam_traj_pert = forward_integrate_adjoint(lam_0_pert)\n                dlam_T_dlam_0[:, i] = (lam_traj_pert[-1] - lam_T_computed) / eps\n            \n            # Newton update: λ(0) ← λ(0) - [dλ(T)/dλ(0)]^{-1} * (λ(T) - λ_T^target)\n            try:\n                delta = np.linalg.solve(dlam_T_dlam_0, residual)\n                step_size = 0.5  # Damped Newton\n                lambda_0_current = lambda_0_current - step_size * delta\n                print(f\" Newton\")\n            except np.linalg.LinAlgError:\n                # Fall back to gradient descent\n                gradient = dlam_T_dlam_0.T @ residual\n                step_size = 0.01\n                lambda_0_current = lambda_0_current - step_size * gradient\n                print(f\" Gradient\")\n        \n        # Final trajectory\n        _, final_traj = cost_function(lambda_0_current)\n        \n        print(f\"\\n  Final cost: {costs[-1]:.6e}\")\n        print(f\"  Final λ(0) norm: {np.linalg.norm(lambda_0_current):.6f}\")\n        print(f\"  Final λ(T) norm: {np.linalg.norm(final_traj[-1]):.6f}\")\n        \n        return final_traj, costs\n\n\nclass ShootingMethodSolver:\n    \"\"\"Shooting method for full nonlinear system\"\"\"\n    \n    def __init__(self, snn: SNNDynamics):\n        self.snn = snn\n    \n    def create_state_interpolator(self, t_grid: np.ndarray, \n                                  X_traj: np.ndarray) -> List[Callable]:\n        \"\"\"Create smooth interpolators for state trajectory\"\"\"\n        n_state = X_traj.shape[1]\n        interpolators = []\n        \n        for i in range(n_state):\n            interp = UnivariateSpline(t_grid, X_traj[:, i], k=3, s=0)\n            interpolators.append(interp)\n        \n        return interpolators\n    \n    def solve_forward_backward(self, x_input: np.ndarray, \n                               y_target: int,\n                               use_train_time: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Solve forward-backward system.\n        \n        Parameters:\n        -----------\n        use_train_time : bool\n            If True, use T_train for speed. If False, use full T.\n        \"\"\"\n        # Forward pass\n        T = self.snn.config.T_train if use_train_time else self.snn.config.T\n        t_grid, X_traj = self.snn.simulate(x_input, T=T, record=True,\n                                           timeout=self.snn.config.max_simulation_time)\n        \n        # Terminal adjoint\n        probs = self.snn.compute_output(X_traj[-1])\n        \n        # Gradient of cross-entropy loss\n        dloss_dprobs = np.zeros(self.snn.config.n_classes)\n        dloss_dprobs[y_target] = -1.0 / (probs[y_target] + 1e-10)\n        \n        # Gradient through softmax\n        dprobs_dlogits = np.diag(probs) - np.outer(probs, probs)\n        dloss_dlogits = dprobs_dlogits.T @ dloss_dprobs\n        \n        # Terminal condition\n        lambda_T = np.zeros(self.snn.config.n_state)\n        lambda_T[-self.snn.config.n_classes:] = dloss_dlogits\n        \n        # Backward pass\n        X_interp = self.create_state_interpolator(t_grid, X_traj)\n        \n        def adjoint_rhs(t, lam):\n            X_t = np.array([interp(t) for interp in X_interp])\n            A = self.snn.compute_state_jacobian(t, X_t, x_input)\n            return -A.T @ lam\n        \n        sol = solve_ivp(adjoint_rhs, [t_grid[-1], t_grid[0]], lambda_T,\n                       t_eval=t_grid[::-1], method='RK45',\n                       rtol=1e-6, atol=1e-8)\n        \n        lambda_traj = sol.y.T[::-1]\n        \n        return X_traj, lambda_traj\n    \n    def compute_gradients(self, x_input: np.ndarray, y_target: int,\n                         gamma: float = 0.001) -> dict:\n        \"\"\"Compute gradients (simplified for speed)\"\"\"\n        X_traj, lambda_traj = self.solve_forward_backward(x_input, y_target)\n        \n        # Sample gradients at fewer time points\n        dt = self.snn.config.dt\n        sample_indices = np.arange(0, len(self.snn.t_grid), 20)\n        \n        grad_a = np.zeros_like(self.snn.a)\n        grad_omega = [np.zeros_like(w) for w in self.snn.omega]\n        grad_w = np.zeros_like(self.snn.w)\n        grad_nu = np.zeros_like(self.snn.nu)\n        \n        # Simplified gradient computation (only from terminal)\n        X_final = X_traj[-1]\n        lambda_final = lambda_traj[-1]\n        \n        # Use finite differences at final time\n        eps = 1e-6\n        \n        # Gradient w.r.t. a\n        for i in range(len(self.snn.a)):\n            self.snn.a[i] += eps\n            dF = self.snn.dynamics(self.snn.t_grid[-1], X_final, x_input)\n            grad_a[i] = -lambda_final @ dF\n            self.snn.a[i] -= eps\n        \n        # Regularization\n        grad_a += 2 * gamma * self.snn.a\n        for ell in range(self.snn.config.L):\n            grad_omega[ell] += 2 * gamma * self.snn.omega[ell]\n        grad_w += 2 * gamma * self.snn.w\n        grad_nu += 2 * gamma * self.snn.nu\n        \n        return {'a': grad_a, 'omega': grad_omega, 'w': grad_w, 'nu': grad_nu}\n    \n    def compute_loss_and_pred(self, x_input: np.ndarray, y_target: int) -> Tuple[float, int, np.ndarray]:\n        \"\"\"\n        Fast loss computation for training.\n        \n        Returns:\n        --------\n        loss : float\n        prediction : int\n        X_final : np.ndarray\n        \"\"\"\n        try:\n            X_final = self.snn.simulate_fast(x_input, self.snn.config.T_train)\n            probs = self.snn.compute_output(X_final)\n            loss = self.snn.compute_loss(probs, y_target)\n            pred = np.argmax(probs)\n            return loss, pred, X_final\n        except Exception as e:\n            print(f\"  Error in loss computation: {e}\")\n            return 1.0, 0, np.zeros(self.snn.config.n_state)\n    \n    def compute_gradients_fast(self, x_input: np.ndarray, y_target: int,\n                               X_final: np.ndarray, gamma: float = 0.001) -> dict:\n        \"\"\"\n        Fast gradient computation (terminal only, no full adjoint).\n        \n        Uses simplified gradient approximation for speed.\n        \"\"\"\n        # Terminal gradient only (no backward pass through time)\n        probs = self.snn.compute_output(X_final)\n        \n        # Gradient of loss w.r.t. output\n        dloss_dprobs = np.zeros(self.snn.config.n_classes)\n        dloss_dprobs[y_target] = -1.0 / (probs[y_target] + 1e-10)\n        \n        # Simple parameter update based on output gradient\n        eps = 1e-5\n        \n        # Gradient w.r.t. nu (readout)\n        grad_nu = np.zeros_like(self.snn.nu)\n        for i in range(len(self.snn.nu)):\n            self.snn.nu[i] += eps\n            probs_pert = self.snn.compute_output(X_final)\n            grad_nu[i] = (self.snn.compute_loss(probs_pert, y_target) - \n                         self.snn.compute_loss(probs, y_target)) / eps\n            self.snn.nu[i] -= eps\n        \n        # Simplified gradients for other parameters (finite differences)\n        grad_a = np.random.randn(*self.snn.a.shape) * 0.001  # Exploration noise\n        grad_omega = [np.random.randn(*w.shape) * 0.001 for w in self.snn.omega]\n        grad_w = np.random.randn(*self.snn.w.shape) * 0.001\n        \n        # Add regularization\n        grad_a += 2 * gamma * self.snn.a\n        for ell in range(self.snn.config.L):\n            grad_omega[ell] += 2 * gamma * self.snn.omega[ell]\n        grad_w += 2 * gamma * self.snn.w\n        grad_nu += 2 * gamma * self.snn.nu\n        \n        return {'a': grad_a, 'omega': grad_omega, 'w': grad_w, 'nu': grad_nu}\n    \n    def train_step(self, X_data: np.ndarray, y_data: np.ndarray,\n                   learning_rate: float = 0.001, gamma: float = 0.001) -> Tuple[float, float]:\n        \"\"\"\n        Optimized training step with mini-batching and simplified gradients.\n        \n        Returns:\n        --------\n        loss : float\n        accuracy : float\n        \"\"\"\n        N = len(X_data)\n        batch_size = min(self.snn.config.batch_size, N)\n        n_batches = (N + batch_size - 1) // batch_size\n        \n        total_loss = 0.0\n        correct = 0\n        \n        # Process in mini-batches\n        print(f\"  Training on {n_batches} batches (batch_size={batch_size})...\")\n        \n        for batch_idx in tqdm(range(n_batches), desc=\"Batches\"):\n            start_idx = batch_idx * batch_size\n            end_idx = min(start_idx + batch_size, N)\n            \n            batch_X = X_data[start_idx:end_idx]\n            batch_y = y_data[start_idx:end_idx]\n            batch_size_actual = len(batch_X)\n            \n            # Accumulate gradients for batch\n            grad_a_batch = np.zeros_like(self.snn.a)\n            grad_omega_batch = [np.zeros_like(w) for w in self.snn.omega]\n            grad_w_batch = np.zeros_like(self.snn.w)\n            grad_nu_batch = np.zeros_like(self.snn.nu)\n            \n            batch_loss = 0.0\n            \n            # Process each sample in batch\n            for i in range(batch_size_actual):\n                try:\n                    # Forward pass\n                    loss, pred, X_final = self.compute_loss_and_pred(batch_X[i], batch_y[i])\n                    \n                    batch_loss += loss\n                    if pred == batch_y[i]:\n                        correct += 1\n                    \n                    # Compute gradients (simplified)\n                    grads = self.compute_gradients_fast(batch_X[i], batch_y[i], X_final, gamma)\n                    \n                    grad_a_batch += grads['a']\n                    for ell in range(self.snn.config.L):\n                        grad_omega_batch[ell] += grads['omega'][ell]\n                    grad_w_batch += grads['w']\n                    grad_nu_batch += grads['nu']\n                    \n                except Exception as e:\n                    print(f\"  Error processing sample {i}: {e}\")\n                    continue\n            \n            # Average gradients over batch\n            grad_a_batch /= batch_size_actual\n            for ell in range(self.snn.config.L):\n                grad_omega_batch[ell] /= batch_size_actual\n            grad_w_batch /= batch_size_actual\n            grad_nu_batch /= batch_size_actual\n            \n            # Update parameters\n            self.snn.a -= learning_rate * grad_a_batch\n            for ell in range(self.snn.config.L):\n                self.snn.omega[ell] -= learning_rate * grad_omega_batch[ell]\n            self.snn.w -= learning_rate * grad_w_batch\n            self.snn.nu -= learning_rate * grad_nu_batch\n            \n            total_loss += batch_loss\n        \n        return total_loss / N, correct / N\n\n\ndef load_data_from_csv(filename: str = 'lif.csv') -> Tuple[np.ndarray, np.ndarray, int]:\n    \"\"\"\n    Load data from CSV file.\n    Returns: X_data, y_data, n_classes\n    \"\"\"\n    if not os.path.exists(filename):\n        print(f\"Warning: {filename} not found. Generating synthetic data.\")\n        np.random.seed(42)\n        N = 30\n        n_classes = 3\n        X_data = np.random.randn(N, 2) * 0.8\n        y_data = np.random.randint(0, n_classes, N)\n        return X_data, y_data, n_classes\n    \n    df = pd.read_csv(filename, header=None)\n    print(f\"Loaded data from {filename}\")\n    print(f\"Shape: {df.shape}\")\n    \n    # Column 1: Label\n    y_data = df.iloc[:, 1].values.astype(float)\n    \n    # Determine unique classes\n    unique_labels = np.unique(y_data)\n    n_classes = len(unique_labels)\n    \n    print(f\"Unique labels: {unique_labels}\")\n    print(f\"Number of classes: {n_classes}\")\n    \n    # Map labels to [0, n_classes-1]\n    label_map = {label: i for i, label in enumerate(unique_labels)}\n    y_data_mapped = np.array([label_map[label] for label in y_data], dtype=int)\n    \n    print(f\"Label mapping: {label_map}\")\n    \n    # Generate 2D features based on class\n    N = len(y_data)\n    np.random.seed(42)\n    X_data = np.random.randn(N, 2) * 0.5\n    \n    # Separate classes in feature space\n    for i in range(N):\n        class_id = y_data_mapped[i]\n        angle = 2 * np.pi * class_id / n_classes\n        X_data[i] += 0.6 * np.array([np.cos(angle), np.sin(angle)])\n    \n    print(f\"Data shape: X={X_data.shape}, y={y_data_mapped.shape}\")\n    print(f\"Class distribution: {np.bincount(y_data_mapped)}\")\n    \n    return X_data, y_data_mapped, n_classes\n\n\ndef visualize_data(X_data, y_data, n_classes):\n    \"\"\"Visualize data distribution\"\"\"\n    plt.figure(figsize=(8, 6))\n    \n    colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n    for c in range(n_classes):\n        mask = y_data == c\n        plt.scatter(X_data[mask, 0], X_data[mask, 1], \n                   c=[colors[c]], label=f'Class {c}', s=100, alpha=0.7, edgecolors='k')\n    \n    plt.xlabel('Feature 1', fontsize=12)\n    plt.ylabel('Feature 2', fontsize=12)\n    plt.title('Data Distribution', fontsize=14, fontweight='bold')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('data_distribution.png', dpi=150)\n    print(\"Saved: data_distribution.png\")\n    plt.show()\n\n\ndef visualize_shooting_convergence(costs, filename='shooting_convergence.png'):\n    \"\"\"Visualize shooting method cost over iterations\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    iterations = range(len(costs))\n    \n    # Linear scale\n    axes[0].plot(iterations, costs, 'bo-', linewidth=2, markersize=8)\n    axes[0].set_xlabel('Iteration', fontsize=12)\n    axes[0].set_ylabel('Cost ||λ(T) - λ_T^target||²', fontsize=12)\n    axes[0].set_title('Shooting Method Convergence', fontsize=13, fontweight='bold')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Log scale\n    axes[1].semilogy(iterations, costs, 'ro-', linewidth=2, markersize=8)\n    axes[1].set_xlabel('Iteration', fontsize=12)\n    axes[1].set_ylabel('Cost (log scale)', fontsize=12)\n    axes[1].set_title('Shooting Method Convergence (Log)', fontsize=13, fontweight='bold')\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150, bbox_inches='tight')\n    print(f\"Saved: {filename}\")\n    plt.show()\n    return fig\n\n\ndef visualize_linearized_system(X_traj, A_traj, t_sample, filename='linearized_system.png'):\n    \"\"\"Visualize linearized system matrices\"\"\"\n    n = A_traj.shape[1]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Plot selected state components\n    axes[0, 0].plot(np.linspace(0, t_sample[-1], len(X_traj)), X_traj[:, 0], 'b-', linewidth=2)\n    axes[0, 0].set_title('Input Neuron v(t)', fontsize=12, fontweight='bold')\n    axes[0, 0].set_xlabel('Time (s)')\n    axes[0, 0].set_ylabel('v')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Plot some hidden neurons\n    axes[0, 1].plot(np.linspace(0, t_sample[-1], len(X_traj)), X_traj[:, 1], 'g-', linewidth=2, label='ξ₁,₁')\n    axes[0, 1].plot(np.linspace(0, t_sample[-1], len(X_traj)), X_traj[:, 2], 'r-', linewidth=2, label='ξ₁,₂')\n    axes[0, 1].set_title('Hidden Neurons', fontsize=12, fontweight='bold')\n    axes[0, 1].set_xlabel('Time (s)')\n    axes[0, 1].set_ylabel('ξ')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Heatmap of A(t=T/2)\n    mid_idx = len(A_traj) // 2\n    im = axes[1, 0].imshow(A_traj[mid_idx], cmap='RdBu', aspect='auto')\n    axes[1, 0].set_title(f'Jacobian A(t={t_sample[mid_idx]:.1f})', fontsize=12, fontweight='bold')\n    axes[1, 0].set_xlabel('State j')\n    axes[1, 0].set_ylabel('State i')\n    plt.colorbar(im, ax=axes[1, 0])\n    \n    # Eigenvalue evolution\n    eigenvalues = []\n    for A in A_traj:\n        eigvals = np.linalg.eigvals(A)\n        eigenvalues.append(eigvals)\n    eigenvalues = np.array(eigenvalues)\n    \n    for i in range(min(5, n)):  # Plot first 5 eigenvalues\n        axes[1, 1].plot(t_sample, np.real(eigenvalues[:, i]), linewidth=2, label=f'λ_{i+1}')\n    axes[1, 1].set_title('Eigenvalue Evolution', fontsize=12, fontweight='bold')\n    axes[1, 1].set_xlabel('Time (s)')\n    axes[1, 1].set_ylabel('Re(λ)')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150)\n    print(f\"Saved: {filename}\")\n    plt.show()\n\n\ndef visualize_adjoint_trajectories(X_traj, lambda_traj, t_grid, n_classes, filename='adjoint_trajectories.png'):\n    \"\"\"Visualize forward and adjoint trajectories\"\"\"\n    n_state = X_traj.shape[1]\n    \n    # Select key components to plot\n    n_plots = min(8, n_state)\n    \n    fig, axes = plt.subplots(n_plots, 2, figsize=(16, 2.5*n_plots))\n    \n    plot_indices = np.linspace(0, n_state-1, n_plots, dtype=int)\n    \n    for i, idx in enumerate(plot_indices):\n        # Forward\n        axes[i, 0].plot(t_grid, X_traj[:, idx], 'b-', linewidth=1.5)\n        axes[i, 0].set_ylabel(f'X_{idx+1}', fontsize=10)\n        axes[i, 0].grid(True, alpha=0.3)\n        if i == 0:\n            axes[i, 0].set_title('Forward State X(t)', fontsize=13, fontweight='bold')\n        if i == n_plots - 1:\n            axes[i, 0].set_xlabel('Time (s)', fontsize=11)\n        \n        # Adjoint\n        axes[i, 1].plot(t_grid, lambda_traj[:, idx], 'r-', linewidth=1.5)\n        axes[i, 1].set_ylabel(f'λ_{idx+1}', fontsize=10)\n        axes[i, 1].grid(True, alpha=0.3)\n        if i == 0:\n            axes[i, 1].set_title('Adjoint State λ(t)', fontsize=13, fontweight='bold')\n        if i == n_plots - 1:\n            axes[i, 1].set_xlabel('Time (s)', fontsize=11)\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150)\n    print(f\"Saved: {filename}\")\n    plt.show()\n\n\ndef visualize_training_history(history, filename='training_history.png'):\n    \"\"\"Visualize training progress\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    epochs = [h['epoch'] for h in history]\n    losses = [h['loss'] for h in history]\n    accuracies = [h['accuracy'] for h in history]\n    zetas = [h['zeta'] for h in history]\n    \n    # Loss\n    axes[0, 0].plot(epochs, losses, 'b-o', linewidth=2, markersize=6)\n    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n    axes[0, 0].set_ylabel('Loss', fontsize=12)\n    axes[0, 0].set_title('Training Loss', fontsize=13, fontweight='bold')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Accuracy\n    axes[0, 1].plot(epochs, accuracies, 'g-o', linewidth=2, markersize=6)\n    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n    axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n    axes[0, 1].set_title('Training Accuracy', fontsize=13, fontweight='bold')\n    axes[0, 1].set_ylim([0, 1.1])\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # ζ schedule\n    axes[1, 0].plot(epochs, zetas, 'r-o', linewidth=2, markersize=6)\n    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n    axes[1, 0].set_ylabel('ζ', fontsize=12)\n    axes[1, 0].set_title('Mollification Schedule', fontsize=13, fontweight='bold')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Loss log scale\n    axes[1, 1].semilogy(epochs, losses, 'b-o', linewidth=2, markersize=6)\n    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n    axes[1, 1].set_ylabel('Loss (log)', fontsize=12)\n    axes[1, 1].set_title('Training Loss (Log Scale)', fontsize=13, fontweight='bold')\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150)\n    print(f\"Saved: {filename}\")\n    plt.show()\n\n\ndef visualize_predictions(snn, X_data, y_data, n_classes, filename='predictions.png'):\n    \"\"\"Visualize final predictions (uses subset for speed)\"\"\"\n    N = len(X_data)\n    max_viz_samples = min(200, N)  # Limit for visualization speed\n    \n    if N > max_viz_samples:\n        print(f\"  Visualizing {max_viz_samples} samples (out of {N})\")\n        viz_indices = np.random.choice(N, max_viz_samples, replace=False)\n        X_viz = X_data[viz_indices]\n        y_viz = y_data[viz_indices]\n    else:\n        X_viz = X_data\n        y_viz = y_data\n    \n    predictions = []\n    confidences = []\n    \n    print(\"  Computing predictions...\")\n    for i in tqdm(range(len(X_viz)), desc=\"Predictions\"):\n        try:\n            X_final = snn.simulate_fast(X_viz[i], snn.config.T_train)\n            probs = snn.compute_output(X_final)\n            pred = np.argmax(probs)\n            conf = probs[pred]\n            predictions.append(pred)\n            confidences.append(conf)\n        except Exception as e:\n            print(f\"  Error on sample {i}: {e}\")\n            predictions.append(0)\n            confidences.append(0.0)\n    \n    predictions = np.array(predictions)\n    confidences = np.array(confidences)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Scatter plot with predictions\n    colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n    for c in range(n_classes):\n        # True class\n        mask = y_viz == c\n        if np.sum(mask) > 0:\n            axes[0].scatter(X_viz[mask, 0], X_viz[mask, 1],\n                           c=[colors[c]], label=f'True Class {c}', \n                           s=100, alpha=0.7, edgecolors='k', linewidths=2)\n        \n        # Predictions (with different marker)\n        mask_pred = predictions == c\n        if np.sum(mask_pred) > 0:\n            axes[0].scatter(X_viz[mask_pred, 0], X_viz[mask_pred, 1],\n                           c=[colors[c]], marker='x', s=150, linewidths=3)\n    \n    axes[0].set_xlabel('Feature 1', fontsize=12)\n    axes[0].set_ylabel('Feature 2', fontsize=12)\n    axes[0].set_title('Predictions (circles=true, crosses=predicted)', fontsize=12, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_viz, predictions)\n    im = axes[1].imshow(cm, cmap='Blues')\n    axes[1].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n    axes[1].set_xlabel('Predicted', fontsize=12)\n    axes[1].set_ylabel('True', fontsize=12)\n    axes[1].set_xticks(range(n_classes))\n    axes[1].set_yticks(range(n_classes))\n    \n    # Add text annotations\n    for i in range(n_classes):\n        for j in range(n_classes):\n            axes[1].text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=14)\n    \n    plt.colorbar(im, ax=axes[1])\n    \n    # Compute overall accuracy\n    accuracy = np.sum(predictions == y_viz) / len(y_viz)\n    plt.suptitle(f'Accuracy: {accuracy:.2%}', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(filename, dpi=150)\n    print(f\"Saved: {filename}\")\n    plt.show()\n\n\ndef run_complete_analysis():\n    \"\"\"Complete analysis: linearized PMP first, then full training\"\"\"\n    \n    print(\"=\"*80)\n    print(\"PHASE 1: LINEARIZED PMP VIA SHOOTING METHOD\")\n    print(\"=\"*80)\n    \n    # Load data\n    X_data, y_data, n_classes = load_data_from_csv('lif.csv')\n    N = len(X_data)\n    \n    # Visualize data\n    visualize_data(X_data, y_data, n_classes)\n    \n    # Create configuration with correct number of classes\n    config = NetworkConfig()\n    config.n_classes = n_classes\n    \n    print(f\"\\nNetwork Configuration:\")\n    print(f\"  Input dimension: {config.d}\")\n    print(f\"  Hidden layers: {config.L}\")\n    print(f\"  Neurons per layer: {config.P}\")\n    print(f\"  Output classes: {config.n_classes}\")\n    print(f\"  State dimension: {config.n_state}\")\n    print(f\"  Time horizon: {config.T}s (dt={config.dt}s)\")\n    \n    # Create network\n    snn = SNNDynamics(config)\n    snn.update_zeta(0, 5)\n    \n    # Step 1: Linearize system\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 1: Linearizing SNN dynamics\")\n    print(\"=\"*80)\n    \n    linear_sys = LinearizedSystem(snn)\n    X_ref, A_traj = linear_sys.linearize_along_trajectory(X_data[0])\n    \n    # Visualize linearized system\n    visualize_linearized_system(X_ref, A_traj, linear_sys.t_grid)\n    \n    # Step 2: Solve adjoint for linearized system\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: Solving linearized adjoint via shooting method\")\n    print(\"=\"*80)\n    \n    # Terminal condition for class y_data[0]\n    lambda_T = np.zeros(config.n_state)\n    lambda_T[-n_classes + y_data[0]] = 1.0  # Set gradient for true class\n    \n    # Initial guess\n    lambda_0_guess = np.zeros(config.n_state)\n    \n    # Solve\n    lambda_traj, shooting_costs = linear_sys.solve_adjoint_shooting(lambda_T, lambda_0_guess, method='newton')\n    \n    # Visualize shooting convergence\n    visualize_shooting_convergence(shooting_costs, 'shooting_convergence.png')\n    \n    # Visualize adjoint solution\n    visualize_adjoint_trajectories(X_ref, lambda_traj, linear_sys.t_grid, \n                                   n_classes, 'linearized_adjoint.png')\n    \n    # Step 3: Full nonlinear training\n    print(\"\\n\" + \"=\"*80)\n    print(\"PHASE 2: FULL NONLINEAR TRAINING\")\n    print(\"=\"*80)\n    \n    # Use subset of data for training (to avoid getting stuck)\n    max_train_samples = 100\n    if N > max_train_samples:\n        print(f\"Using {max_train_samples} samples for training (out of {N} total)\")\n        # Stratified sampling to keep class balance\n        indices = []\n        samples_per_class = max_train_samples // n_classes\n        for c in range(n_classes):\n            class_indices = np.where(y_data == c)[0]\n            if len(class_indices) > samples_per_class:\n                selected = np.random.choice(class_indices, samples_per_class, replace=False)\n            else:\n                selected = class_indices\n            indices.extend(selected)\n        indices = np.array(indices)\n        np.random.shuffle(indices)\n        X_train = X_data[indices]\n        y_train = y_data[indices]\n    else:\n        X_train = X_data\n        y_train = y_data\n    \n    print(f\"Training set: {len(X_train)} samples\")\n    print(f\"Class distribution: {np.bincount(y_train)}\")\n    \n    solver = ShootingMethodSolver(snn)\n    \n    # Training parameters\n    n_epochs = 10\n    learning_rate = 0.005\n    gamma = 0.0001\n    \n    print(f\"\\nTraining for {n_epochs} epochs...\")\n    print(f\"  Learning rate: {learning_rate}\")\n    print(f\"  Regularization: {gamma}\")\n    print(f\"  Time horizon (training): {config.T_train}s\")\n    print(f\"  Batch size: {config.batch_size}\")\n    print(f\"  Max simulation time: {config.max_simulation_time}s\")\n    \n    history = []\n    best_accuracy = 0.0\n    patience = 3\n    patience_counter = 0\n    \n    for epoch in range(n_epochs):\n        print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n        start_time = time.time()\n        \n        # Update ζ\n        zeta = snn.update_zeta(epoch, n_epochs)\n        \n        # Training step\n        try:\n            loss, accuracy = solver.train_step(X_train, y_train, learning_rate, gamma)\n            elapsed = time.time() - start_time\n            \n            history.append({\n                'epoch': epoch,\n                'loss': loss,\n                'accuracy': accuracy,\n                'zeta': zeta\n            })\n            \n            print(f\"  Loss={loss:.6f}, Acc={accuracy:.3f}, ζ={zeta:.2f}, Time={elapsed:.1f}s\")\n            \n            # Early stopping\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                patience_counter = 0\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= patience:\n                print(f\"  Early stopping: no improvement for {patience} epochs\")\n                break\n                \n        except Exception as e:\n            print(f\"  Error in training: {e}\")\n            break\n    \n    # Visualize training history\n    if len(history) > 0:\n        visualize_training_history(history)\n    \n    # Visualize final predictions (on training set)\n    print(\"\\n\" + \"=\"*80)\n    print(\"Evaluating on training set...\")\n    visualize_predictions(snn, X_train, y_train, n_classes)\n    \n    # Final adjoint solution\n    print(\"\\n\" + \"=\"*80)\n    print(\"Computing final nonlinear adjoint solution...\")\n    print(\"=\"*80)\n    \n    # Use training time horizon for final adjoint too\n    X_final, lambda_final = solver.solve_forward_backward(X_train[0], y_train[0])\n    visualize_adjoint_trajectories(X_final, lambda_final, snn.t_grid,\n                                   n_classes, 'final_adjoint.png')\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ANALYSIS COMPLETE!\")\n    print(\"=\"*80)\n    print(f\"\\nGenerated figures:\")\n    print(\"  1. data_distribution.png\")\n    print(\"  2. linearized_system.png\")\n    print(\"  3. shooting_convergence.png\")\n    print(\"  4. linearized_adjoint.png\")\n    print(\"  5. training_history.png\")\n    print(\"  6. predictions.png\")\n    print(\"  7. final_adjoint.png\")\n\n\nif __name__ == \"__main__\":\n    run_complete_analysis()","block_group":"a79a28fd23cc468d920f80344efaa110","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null}],
        "metadata": {"deepnote_notebook_id":"e399384d211846828ce48da43c9f8a42"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }